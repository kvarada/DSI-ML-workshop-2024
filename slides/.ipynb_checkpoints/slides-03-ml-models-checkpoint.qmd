---
title: "Supervised Machine Learning Models"
format: 
    revealjs:
      smaller: true
      center: true
---

## Supervised Learning
\

In the next section, we will briefly introduce a few types of machine learning models that are often used for supervised learning tasks.

We will discuss some basic intuition around how they work, and also discuss their relative strengths and shortcomings.

# Tree-based models

## Tree-based models

![](img/actual-tree.jpg)

We have seen that decision trees are prone to overfitting. There are several models that extend the basic idea of using decision trees.

## Random Forest

![](img/random-forest.png)

Train an ensemble of distinct decision trees.

## Random Forest
\

Each tree trains on a random sample of the data. Some times the features used to split are also randomized at each node.

Idea: Individual trees still learn noise in the data, but the noise should "average out" over the ensemble.

## Gradient Boosted Trees

![](img/boost.png)

Each tree tries to "correct" or improve the previous tree's prediction.

## Tree-Based Models
\ 

Random Forest, XGBoost, etc are all easily available as "out-of-the box solutions".

Pros: 

* Perform well on a variety of tasks
* Random forest in particular are easy to train and robust to outliers.

Cons:

* Not always interpretable
* Not good at handling sparse data
* Can also still overfit.

# Analogy-based models

## Analogy-based models

![](img/us-map.png)

Can you guess what this dataset is?

## Analogy-based models

![](img/us-map-dot.jpg)

How would you classify the green dot?

## Analogy-based models
\

Idea: predict on new data based on "similar" examples in the training data.

## *K*-Nearest-Neighbour Classifier
\

Find the *K* nearest neighbours of an example, and predict whichever class was most common among them.

'*K*' is a hyperparameter. Choosing *K=1* is likely to overfit. If the dataset has *N* examples, setting *K=N* just predicts the mode (dummy classifier).

No training phase, but the model can get arbitrarily large (and take very long to make predictions).

## SVM with RBF kernel
\ 

Another 'analogy-based' classification method.

The model stores examples with positive and negative weights. Being close to a positive example makes your label more likely to be positive.

Can lead to "smoother" decision boundaries than K-NNs, and potentially to a smaller trained model.

## KNNs and SVMs
\ 
![](img/knn-vs-svm.png)

## Analogy-based Models
\

Pros:

* Do not need to make assumptions about the underlying data
* Given enough data, should pretty much always work.

Cons:

* *Enough* data can mean ... a lot
* Computing distances is time-consuming for large datasets
* Can't really interpret the model's decisions.

# Linear models

## Linear models
\

![](img/lse)

Many of you might be familiar with least-squares regression. We find the line of best fit by minimizing the 'squared error' of the predictions.

## Linear Models
\

![](img/quadreg.png)

We can use the same idea (and some neat tricks) to fit higher degree polynomials, or any chosen basis of functions.

## Linear Models
\ 
![](img/outlierbad.png)

Squared Error is very sensitive to outliers. Far-away points contribute a very large squared error, and even relatively few points can affect the outcome.

## Linear Models
\ 
![](img/outliergood.png)

We can use other notions of "best fit". Using absolute error makes the model more resistant to outliers!

## Linear Classifiers
\

We can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a "probability".

In *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data "most likely".

## Linear Classifiers

![](img/logistic.png)

Logistic Regression predicts a *linear* decision boundary.

## Linear Classifiers
\

Pros:

* Easy to train and to interpret
* Widely applicable despite some strong assumptions
* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.

Cons:

* Strong assumptions
* Linear decision boundaries for classifiers
* Correlated features can cause problems

## A Look Ahead
\

Support Vector Machines (SVM) are also linear classifiers.

The reason we saw a *non-linear* decision boundary earlier was the use of the *RBF* kernel, which applies a certain non-linear transformation to the features.

Even if our data is not linearly separable, there could be a good choice of feature transform out there that *makes* it linearly separable.

##

Wouldn't it be nice if we could train a machine learning model to find such a transform?
