[
  {
    "objectID": "slides/slides-04-deep-learning.html#learning-outcomes",
    "href": "slides/slides-04-deep-learning.html#learning-outcomes",
    "title": "Deep Learning",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nFrom this module, you will be able to\n\nExplain the role of neural networks in machine learning, including their advantages and disadvantages.\nDiscuss why traditional methods are less effective for image data.\nGain a high-level understanding of transfer learning.\nDifferentiate between image classification and object detection."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#introduction-to-neural-networks",
    "href": "slides/slides-04-deep-learning.html#introduction-to-neural-networks",
    "title": "Deep Learning",
    "section": "Introduction to neural networks",
    "text": "Introduction to neural networks\n\n\n\nNeural networks are very popular these days under the name deep learning.\nNeural networks apply a sequence of transformations on your input data.\nThey can be viewed a generalization of linear models where we apply a series of transformations.\nHere is graphical representation of a logistic regression model.\nWe have 4 features: x[0], x[1], x[2], x[3]"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#adding-a-layer-of-transformations",
    "href": "slides/slides-04-deep-learning.html#adding-a-layer-of-transformations",
    "title": "Deep Learning",
    "section": "Adding a layer of transformations",
    "text": "Adding a layer of transformations\n\n\n\nBelow we are adding one “layer” of transformations in between features and the target.\nWe are repeating the the process of computing the weighted sum multiple times.\n\nThe hidden units (e.g., h[1], h[2], …) represent the intermediate processing steps."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#one-more-layer-of-transformations",
    "href": "slides/slides-04-deep-learning.html#one-more-layer-of-transformations",
    "title": "Deep Learning",
    "section": "One more layer of transformations",
    "text": "One more layer of transformations\n\n\n\nNow we are adding one more layer of transformations."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#neural-networks",
    "href": "slides/slides-04-deep-learning.html#neural-networks",
    "title": "Deep Learning",
    "section": "Neural networks",
    "text": "Neural networks\n\n\n\nA neural network is a model that’s sort of like its own pipeline\n\nIt involves a series of transformations (“layers”) internally.\nThe output is the prediction.\n\nWith a neural net, you specify the number of features after each transformation.\n\nIn the above, it goes from 4 to 3 to 3 to 1.\n\nTo make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node.\nNeural network = neural net\nDeep learning ~ using neural networks"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#why-neural-networks",
    "href": "slides/slides-04-deep-learning.html#why-neural-networks",
    "title": "Deep Learning",
    "section": "Why neural networks?",
    "text": "Why neural networks?\n\n\n\nThey can learn very complex functions.\n\nThe fundamental tradeoff is primarily controlled by the number of layers and layer sizes.\nMore layers / bigger layers –&gt; more complex model.\nYou can generally get a model that will not underfit.\n\nThe work really well for structured data:\n\n1D sequence, e.g. timeseries, language\n2D image\n3D image or video\n\nThey’ve had some incredible successes in the last 10 years.\nTransfer learning (coming later today) is really useful."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#why-not-neural-networks",
    "href": "slides/slides-04-deep-learning.html#why-not-neural-networks",
    "title": "Deep Learning",
    "section": "Why not neural networks?",
    "text": "Why not neural networks?\n\n\n\nOften they require a lot of data.\nThey require a lot of compute time, and, to be faster, specialized hardware called GPUs.\nThey have huge numbers of hyperparameters are a huge pain to tune.\n\nThink of each layer having hyperparameters, plus some overall hyperparameters.\nBeing slow compounds this problem.\n\nThey are not interpretable.\nI don’t recommend training them on your own without further training\n\nGood news\n\nYou don’t have to train your models from scratch in order to use them.\nI’ll show you some ways to use neural networks without training them yourselves."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#deep-learning-software",
    "href": "slides/slides-04-deep-learning.html#deep-learning-software",
    "title": "Deep Learning",
    "section": "Deep learning software",
    "text": "Deep learning software\n\n\nThe current big players are:\n\nPyTorch\nTensorFlow\n\nBoth are heavily used in industry. If interested, see comparison of deep learning software."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#introduction-to-computer-vision",
    "href": "slides/slides-04-deep-learning.html#introduction-to-computer-vision",
    "title": "Deep Learning",
    "section": "Introduction to computer vision",
    "text": "Introduction to computer vision\n\n\n\nComputer vision refers to understanding images/videos, usually using ML/AI. It has many tasks of interest:\n\nimage classification: is this a cat or a dog?\nobject localization: where is the cat in this image?\nobject detection: What are the various objects in the image?\ninstance segmentation: What are the shapes of these various objects in the image?\nand much more…\n\n\n\n\nIn the last decade this field has been dominated by deep learning. We will explore image classification and object detection."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#image-classification",
    "href": "slides/slides-04-deep-learning.html#image-classification",
    "title": "Deep Learning",
    "section": "Image classification",
    "text": "Image classification\n\n\nHave you used search in Google Photos? You can search for “my photos of cat” and it will retrieve photos from your libraries containing cats. This can be done using image classification, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#image-classification-1",
    "href": "slides/slides-04-deep-learning.html#image-classification-1",
    "title": "Deep Learning",
    "section": "Image classification",
    "text": "Image classification\n\n\nImage classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#convolutional-neural-networks",
    "href": "slides/slides-04-deep-learning.html#convolutional-neural-networks",
    "title": "Deep Learning",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\n\n\nA significant advancement in image classification was the application of convolutional neural networks (ConvNets or CNNs) to this problem.\nImageNet Classification with Deep Convolutional Neural Networks\nAchieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#pre-trained-models",
    "href": "slides/slides-04-deep-learning.html#pre-trained-models",
    "title": "Deep Learning",
    "section": "Pre-trained models",
    "text": "Pre-trained models\n\n\n\nIn practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\nInstead, a common practice is to download a pre-trained model and fine tune it for your task. This is called transfer learning.\nTransfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\nIt refers to using a model already trained on one task as a starting point for learning to perform another task"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#pre-trained-models-1",
    "href": "slides/slides-04-deep-learning.html#pre-trained-models-1",
    "title": "Deep Learning",
    "section": "Pre-trained models",
    "text": "Pre-trained models\n\n\n\nThere are many famous deep learning architectures out there that have been very successful across a wide range of problems, e.g.: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\nMany of these models have been pre-trained on famous datasets like ImageNet [1, 2]"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#imagenet",
    "href": "slides/slides-04-deep-learning.html#imagenet",
    "title": "Deep Learning",
    "section": "ImageNet",
    "text": "ImageNet\n\n\n\nImageNet is an image dataset that became a very popular benchmark in the field ~12 years ago.\nCurrently contains ~14 million labelled images with ~21,841 categories\n\nThere are various versions with different number of images and classes\n\nILSVRC, a popular annual competition in computer vision, uses a smaller subset of ImageNet. This subset consists of about 1.2 million training images, 50,000 validation images, and 150,000 testing images across 1,000 categories.\n\nWikipedia article on ImageNet"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#imagenet-classes",
    "href": "slides/slides-04-deep-learning.html#imagenet-classes",
    "title": "Deep Learning",
    "section": "ImageNet classes",
    "text": "ImageNet classes\n\n\n\nHere are some example classes.\n\n\nwith open(\"data/imagenet_classes.txt\") as f:\n    classes = [line.strip() for line in f.readlines()]\nclasses[100:110]\n\n['black swan, Cygnus atratus',\n 'tusker',\n 'echidna, spiny anteater, anteater',\n 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n 'wallaby, brush kangaroo',\n 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n 'wombat',\n 'jellyfish',\n 'sea anemone, anemone',\n 'brain coral']"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#transfer-learning",
    "href": "slides/slides-04-deep-learning.html#transfer-learning",
    "title": "Deep Learning",
    "section": "Transfer learning",
    "text": "Transfer learning\n\n\n\nThe idea of transfer learning is instead of developing a machine learning model from scratch, you use these available pre-trained models for your tasks either directly or by fine tuning them.\nThere are three common ways to use transfer learning in computer vision\n\nUsing pre-trained models out-of-the-box\nUsing pre-trained models as feature extractor and training your own model with these features\nStarting with weights of pre-trained models and fine-tuning the weights for your task.\n\nWe will explore the first two approaches."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box",
    "title": "Deep Learning",
    "section": "Using pre-trained models out-of-the-box",
    "text": "Using pre-trained models out-of-the-box\n\n\n\n\n\nLet’s first try one of these models and apply it to our own problem right out of the box."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-1",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-1",
    "title": "Deep Learning",
    "section": "Using pre-trained models out-of-the-box",
    "text": "Using pre-trained models out-of-the-box\n\n\n\nWe can easily download famous models using the torchvision.models module. All models are available with pre-trained weights (based on ImageNet’s 224 x 224 images)\nWe used a pre-trained model vgg16 which is trained on the ImageNet data.\nWe preprocess the given image.\nWe get prediction from this pre-trained model on a given image along with prediction probabilities.\n\nFor a given image, this model will spit out one of the 1000 classes from ImageNet."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-2",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-2",
    "title": "Deep Learning",
    "section": "Using pre-trained models out-of-the-box",
    "text": "Using pre-trained models out-of-the-box\n\nLet’s predict labels with associated probabilities for unseen images\n\n\n\n\n\n\n\n\n\n\n\n                         Class  Probability score\n                     tiger cat              0.353\n              tabby, tabby cat              0.207\n               lynx, catamount              0.050\nPembroke, Pembroke Welsh corgi              0.046\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.983\n                  leopard, Panthera pardus              0.012\njaguar, panther, Panthera onca, Felis onca              0.004\n       snow leopard, ounce, Panthera uncia              0.001\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                   Class  Probability score\n                                 macaque              0.714\npatas, hussar monkey, Erythrocebus patas              0.122\n      proboscis monkey, Nasalis larvatus              0.098\n                   guenon, guenon monkey              0.017\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                        Class  Probability score\nWalker hound, Walker foxhound              0.580\n             English foxhound              0.091\n                  EntleBucher              0.080\n                       beagle              0.065\n--------------------------------------------------------------"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-3",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-3",
    "title": "Deep Learning",
    "section": "Using pre-trained models out-of-the-box",
    "text": "Using pre-trained models out-of-the-box\n\n\n\nWe got these predictions without “doing the ML ourselves”.\nWe are using pre-trained vgg16 model which is available in torchvision.\n\ntorchvision has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n\nMany of these models have been pre-trained on famous datasets like ImageNet.\nSo if we use them out-of-the-box, they will give us one of the ImageNet classes as classification."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-4",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-4",
    "title": "Deep Learning",
    "section": "Using pre-trained models out-of-the-box",
    "text": "Using pre-trained models out-of-the-box\n\n\n\nLet’s try some images which are unlikely to be there in ImageNet.\nIt’s not doing very well here because ImageNet doesn’t have proper classes for these images.\n\n\n\n\n\n\n\n\n\n\n\n         Class  Probability score\ncucumber, cuke              0.146\n         plate              0.117\n     guacamole              0.099\n  Granny Smith              0.091\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                           Class  Probability score\nchocolate sauce, chocolate syrup              0.609\n                        espresso              0.039\n             ice cream, icecream              0.037\n                     face powder              0.030\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                      Class  Probability score\n                                        fig              0.637\n                                pomegranate              0.193\ngrocery store, grocery, food market, market              0.041\n                                      crate              0.023\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                               Class  Probability score\n                                         toilet seat              0.171\n                                          safety pin              0.060\nbannister, banister, balustrade, balusters, handrail              0.039\n                                              bubble              0.035\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                    Class  Probability score\nrobin, American robin, Turdus migratorius              0.203\n                                  jacamar              0.156\n                                   bulbul              0.122\n      brambling, Fringilla montifringilla              0.103\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                         Class  Probability score\ngoldfinch, Carduelis carduelis              0.585\n                     bee eater              0.219\n                        toucan              0.038\n                      hornbill              0.026\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                  Class  Probability score\n                   vase              0.078\n                thimble              0.074\n             plate rack              0.049\nsaltshaker, salt shaker              0.047\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                      Class  Probability score\n           pizza, pizza pie              0.998\nfrying pan, frypan, skillet              0.001\n                     potpie              0.000\n                French loaf              0.000\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n              Class  Probability score\n     patio, terrace              0.213\n           fountain              0.164\nlakeside, lakeshore              0.097\n            sundial              0.088\n--------------------------------------------------------------"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-5",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-out-of-the-box-5",
    "title": "Deep Learning",
    "section": "Using pre-trained models out-of-the-box",
    "text": "Using pre-trained models out-of-the-box\n\n\n\nHere we are using pre-trained models out-of-the-box.\nCan we use pre-trained models for our own classification problem with our classes?\nYes!! We have two options here:\n\nAdd some extra layers to the pre-trained network to suit our particular task\nPass training data through the network and save the output to use as features for training some other model"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features",
    "title": "Deep Learning",
    "section": "Using pre-trained models to extract features",
    "text": "Using pre-trained models to extract features\n\n\n\nLet’s use pre-trained models to extract features.\nWe will pass our specific data through a pre-trained network to get a feature vector for each example in the data.\nThe feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network.\nYou can think of each layer a transformer applying some transformations on the input received to that later."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features-1",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features-1",
    "title": "Deep Learning",
    "section": "Using pre-trained models to extract features",
    "text": "Using pre-trained models to extract features\n\n\n\nOnce we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest.\nThis classifier will be trained on our classes using feature representations extracted from the pre-trained models.\n\nLet’s try this out.\nIt’s better to train such models with GPU. Since our dataset is quite small, we won’t have problems running it on a CPU."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features-2",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features-2",
    "title": "Deep Learning",
    "section": "Using pre-trained models to extract features",
    "text": "Using pre-trained models to extract features\n\n\nLet’s look at some sample images in the dataset."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#dataset-statistics",
    "href": "slides/slides-04-deep-learning.html#dataset-statistics",
    "title": "Deep Learning",
    "section": "Dataset statistics",
    "text": "Dataset statistics\n\n\nHere is the stat of our toy dataset.\n\n\nClasses: ['beet_salad', 'chocolate_cake', 'edamame', 'french_fries', 'pizza', 'spring_rolls', 'sushi']\nClass count: 40, 38, 40\nSamples: 283\nFirst sample: ('data/food/train/beet_salad/104294.jpg', 0)"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features-3",
    "href": "slides/slides-04-deep-learning.html#using-pre-trained-models-to-extract-features-3",
    "title": "Deep Learning",
    "section": "Using pre-trained models to extract features",
    "text": "Using pre-trained models to extract features\n\n\n\nNow for each image in our dataset, we’ll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#shape-of-the-feature-vector",
    "href": "slides/slides-04-deep-learning.html#shape-of-the-feature-vector",
    "title": "Deep Learning",
    "section": "Shape of the feature vector",
    "text": "Shape of the feature vector\n\n\n\nNow we have extracted feature vectors for all examples. What’s the shape of these features?\n\n\n\ntorch.Size([283, 1024])\n\n\n\nThe size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.\n\n\nSource"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#a-feature-vector-given-by-densenet",
    "href": "slides/slides-04-deep-learning.html#a-feature-vector-given-by-densenet",
    "title": "Deep Learning",
    "section": "A feature vector given by densenet",
    "text": "A feature vector given by densenet\n \n\nLet’s examine the feature vectors.\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n\n\n\n\n0\n0.000397\n0.006987\n0.002080\n0.001633\n0.164544\n1.284627\n0.000326\n0.003747\n0.019033\n0.000411\n...\n2.017400\n0.265346\n4.620863\n0.285282\n0.526915\n0.428759\n2.960554\n1.577721\n2.150136\n0.735927\n\n\n1\n0.000639\n0.004926\n0.000985\n0.001324\n0.145076\n0.558028\n0.000775\n0.002603\n0.181858\n0.000249\n...\n0.158792\n0.380575\n1.309545\n0.009790\n0.007745\n1.773695\n0.298153\n0.697784\n0.462329\n1.448314\n\n\n2\n0.000109\n0.006808\n0.003910\n0.002917\n0.137208\n0.702670\n0.000546\n0.003634\n0.386037\n0.000179\n...\n1.730925\n0.139263\n0.384295\n1.754404\n0.267863\n0.480649\n0.591291\n3.880836\n1.436774\n0.856637\n\n\n3\n0.000350\n0.002965\n0.001391\n0.000091\n0.119358\n0.032185\n0.000413\n0.004407\n0.000000\n0.000120\n...\n0.866084\n0.118614\n0.404703\n0.139143\n0.588674\n0.326834\n0.277743\n2.369125\n0.295955\n0.206888\n\n\n4\n0.000350\n0.003532\n0.002254\n0.002841\n0.102675\n0.382482\n0.000436\n0.004212\n0.816037\n0.000554\n...\n0.010685\n1.147245\n1.892539\n1.929401\n0.694761\n0.744420\n1.705220\n1.241349\n2.351264\n0.054507\n\n\n\n\n5 rows × 1024 columns\n\n\n\n\nThe features are hard to interpret but they have some important information about the images which can be useful for classification."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#logistic-regression-with-the-extracted-features",
    "href": "slides/slides-04-deep-learning.html#logistic-regression-with-the-extracted-features",
    "title": "Deep Learning",
    "section": "Logistic regression with the extracted features",
    "text": "Logistic regression with the extracted features\n\n\n\nLet’s try out logistic regression on these extracted features.\n\n\n\nTraining score:  1.0\n\n\n\n\nValidation score:  0.8507462686567164\n\n\n\nThis is great accuracy for so little data and little effort!!!"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#sample-predictions",
    "href": "slides/slides-04-deep-learning.html#sample-predictions",
    "title": "Deep Learning",
    "section": "Sample predictions",
    "text": "Sample predictions\n\n\nLet’s examine some sample predictions on the validation set."
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#object-detection-using-yolo",
    "href": "slides/slides-04-deep-learning.html#object-detection-using-yolo",
    "title": "Deep Learning",
    "section": "Object detection using YOLO",
    "text": "Object detection using YOLO\n\n\n\nAnother useful task and tool to know is object detection using YOLO model.\nLet’s identify objects in a sample image using a pretrained model called YOLO5\n\n\n\nrequirements: Ultralytics requirements ['gitpython&gt;=3.1.30', 'pillow&gt;=10.3.0', 'requests&gt;=2.32.0', 'setuptools&gt;=70.0.0'] not found, attempting AutoUpdate...\nRequirement already satisfied: gitpython&gt;=3.1.30 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.1.43)\nRequirement already satisfied: pillow&gt;=10.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (10.4.0)\nRequirement already satisfied: requests&gt;=2.32.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.32.3)\nRequirement already satisfied: setuptools&gt;=70.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (72.2.0)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gitpython&gt;=3.1.30) (4.0.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests&gt;=2.32.0) (1.26.6)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests&gt;=2.32.0) (2021.5.30)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests&gt;=2.32.0) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests&gt;=2.32.0) (3.2)\nRequirement already satisfied: smmap&lt;5,&gt;=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&gt;=3.1.30) (4.0.0)\n\nrequirements: AutoUpdate success ✅ 3.7s, installed 4 packages: ['gitpython&gt;=3.1.30', 'pillow&gt;=10.3.0', 'requests&gt;=2.32.0', 'setuptools&gt;=70.0.0']\nrequirements: ⚠️ Restart runtime or rerun command for updates to take effect"
  },
  {
    "objectID": "slides/slides-04-deep-learning.html#summary",
    "href": "slides/slides-04-deep-learning.html#summary",
    "title": "Deep Learning",
    "section": "Summary",
    "text": "Summary\n\n\n\nNeural networks are a flexible class of models.\n\nThey are hard to train\nThey are particular powerful for structured input like images, videos, audio, etc.\n\nThe good news is we can use pre-trained neural networks.\n\nThis saves us a huge amount of time/cost/effort/resources.\nWe can use these pre-trained networks directly or use them as feature transformers."
  },
  {
    "objectID": "slides/slides-01-intro.html#welcome-to-the-workshop",
    "href": "slides/slides-01-intro.html#welcome-to-the-workshop",
    "title": "Welcome!",
    "section": "Welcome to the workshop!",
    "text": "Welcome to the workshop!\n\nFind a seat where you can see the screen!\nYou will be discussing concepts with your neighbours."
  },
  {
    "objectID": "slides/slides-01-intro.html#meet-us",
    "href": "slides/slides-01-intro.html#meet-us",
    "title": "Welcome!",
    "section": "Meet us",
    "text": "Meet us\n\nVarada\nPrajeet\nYulia\nTony"
  },
  {
    "objectID": "slides/slides-01-intro.html#learning-goals-of-the-workshop",
    "href": "slides/slides-01-intro.html#learning-goals-of-the-workshop",
    "title": "Welcome!",
    "section": "Learning goals of the workshop",
    "text": "Learning goals of the workshop\n\nThe primary learning goal of the workshop is to familiarize yourself with the fundamentals of machine learning and its various types. This knowledge will help you determine which type of machine learning, or whether a non-machine learning approach, would be most appropriate for your problem."
  },
  {
    "objectID": "workshop-07.html",
    "href": "workshop-07.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "workshop-07.html#slides",
    "href": "workshop-07.html#slides",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "workshop-07.html#outline",
    "href": "workshop-07.html#outline",
    "title": "Unsupervised Learning",
    "section": "Outline",
    "text": "Outline\n\nClustering\nDimensionality Reduction\nRecommendation systems",
    "crumbs": [
      "Workshop",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "workshop-03.html",
    "href": "workshop-03.html",
    "title": "Machine Learning Models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Machine Learning Models"
    ]
  },
  {
    "objectID": "workshop-03.html#slides",
    "href": "workshop-03.html#slides",
    "title": "Machine Learning Models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Machine Learning Models"
    ]
  },
  {
    "objectID": "workshop-03.html#outline",
    "href": "workshop-03.html#outline",
    "title": "Machine Learning Models",
    "section": "Outline",
    "text": "Outline\n\nMachine learning terminology\nMachine learning fundamentals\nDecision trees",
    "crumbs": [
      "Workshop",
      "Machine Learning Models"
    ]
  },
  {
    "objectID": "workshop-01.html",
    "href": "workshop-01.html",
    "title": "Welcome and getting started",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Welcome and getting started"
    ]
  },
  {
    "objectID": "workshop-01.html#slides",
    "href": "workshop-01.html#slides",
    "title": "Welcome and getting started",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Welcome and getting started"
    ]
  },
  {
    "objectID": "workshop-01.html#outline",
    "href": "workshop-01.html#outline",
    "title": "Welcome and getting started",
    "section": "Outline",
    "text": "Outline\n\nWelcome to the workshop!\nIntroductions and meet your neighbors\nAbout the learning objective of the workshop",
    "crumbs": [
      "Workshop",
      "Welcome and getting started"
    ]
  },
  {
    "objectID": "slides/slides-06-llms.html#learning-outcomes",
    "href": "slides/slides-06-llms.html#learning-outcomes",
    "title": "LLMs",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nFrom this module, you will be able to"
  },
  {
    "objectID": "slides/slides-06-llms.html#introduction-to-large-language-models",
    "href": "slides/slides-06-llms.html#introduction-to-large-language-models",
    "title": "LLMs",
    "section": "Introduction to Large Language Models",
    "text": "Introduction to Large Language Models"
  },
  {
    "objectID": "slides/slides-03-ml-models.html#learning-outcomes",
    "href": "slides/slides-03-ml-models.html#learning-outcomes",
    "title": "Supervised Machine Learning Models",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nFrom this module, you will be able to"
  },
  {
    "objectID": "slides/slides-03-ml-models.html#introduction-to-large-language-models",
    "href": "slides/slides-03-ml-models.html#introduction-to-large-language-models",
    "title": "Supervised Machine Learning Models",
    "section": "Introduction to Large Language Models",
    "text": "Introduction to Large Language Models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nWelcome everyone to the workshop, “Is Machine Learning Suitable for Your Projects?”"
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\ntime\n\n\nDescription\n\n\n\n\n\n\nWelcome and getting started\n\n\n9:00 - 9:10\n\n\nHello and welcome to the workshop!\n\n\n\n\nMachine Learning Fundamentals\n\n\n9:10 - 10:00\n\n\nML fundamentals\n\n\n\n\nMachine Learning Models\n\n\n10:00 - 10:30\n\n\nML models\n\n\n\n\n☕ Break\n\n\n10:30 - 10:40\n\n\n \n\n\n\n\nDeep Learning\n\n\n10:40 - 11:25\n\n\nDeep Learning\n\n\n\n\n☕ Break\n\n\n11:25 - 10:30\n\n\n \n\n\n\n\nUnsupervised Learning\n\n\n11:30 - 12:10\n\n\nDeep Learning\n\n\n\n\n🥗🍴 Lunch\n\n\n12:10 - 13:20\n\n\n \n\n\n\n\nLarge Language Models\n\n\n13:30 - 14:00\n\n\nLLMs\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "workshop.html#schedule",
    "href": "workshop.html#schedule",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\ntime\n\n\nDescription\n\n\n\n\n\n\nWelcome and getting started\n\n\n9:00 - 9:10\n\n\nHello and welcome to the workshop!\n\n\n\n\nMachine Learning Fundamentals\n\n\n9:10 - 10:00\n\n\nML fundamentals\n\n\n\n\nMachine Learning Models\n\n\n10:00 - 10:30\n\n\nML models\n\n\n\n\n☕ Break\n\n\n10:30 - 10:40\n\n\n \n\n\n\n\nDeep Learning\n\n\n10:40 - 11:25\n\n\nDeep Learning\n\n\n\n\n☕ Break\n\n\n11:25 - 10:30\n\n\n \n\n\n\n\nUnsupervised Learning\n\n\n11:30 - 12:10\n\n\nDeep Learning\n\n\n\n\n🥗🍴 Lunch\n\n\n12:10 - 13:20\n\n\n \n\n\n\n\nLarge Language Models\n\n\n13:30 - 14:00\n\n\nLLMs\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Workshop",
      "Overview"
    ]
  },
  {
    "objectID": "slides/slides-05-unsupervised-learning.html#learning-outcomes",
    "href": "slides/slides-05-unsupervised-learning.html#learning-outcomes",
    "title": "Unsupervised Learning",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nFrom this module, you will be able to"
  },
  {
    "objectID": "slides/slides-05-unsupervised-learning.html#introduction-to-unsupervised-learning",
    "href": "slides/slides-05-unsupervised-learning.html#introduction-to-unsupervised-learning",
    "title": "Unsupervised Learning",
    "section": "Introduction to unsupervised learning",
    "text": "Introduction to unsupervised learning"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Our Team",
    "section": "",
    "text": "Varada Kolhatkar\n        Assistant Professor of Teaching, Computer Science\n        \n            Instructor\n        \n        \n    \n\n    \n        \n        Prajeet Bajpai\n        Teaching Postdoctoral Fellow\n        \n            Instructor\n        \n        \n    \n\n\n\nNo matching items"
  },
  {
    "objectID": "workshop-02.html",
    "href": "workshop-02.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Machine Learning Fundamentals"
    ]
  },
  {
    "objectID": "workshop-02.html#slides",
    "href": "workshop-02.html#slides",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Machine Learning Fundamentals"
    ]
  },
  {
    "objectID": "workshop-02.html#outline",
    "href": "workshop-02.html#outline",
    "title": "Machine Learning Fundamentals",
    "section": "Outline",
    "text": "Outline\n\nMachine learning terminology\nMachine learning fundamentals\nDecision trees",
    "crumbs": [
      "Workshop",
      "Machine Learning Fundamentals"
    ]
  },
  {
    "objectID": "workshop-05.html",
    "href": "workshop-05.html",
    "title": "Deep Learning",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Deep Learning"
    ]
  },
  {
    "objectID": "workshop-05.html#slides",
    "href": "workshop-05.html#slides",
    "title": "Deep Learning",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Deep Learning"
    ]
  },
  {
    "objectID": "workshop-05.html#outline",
    "href": "workshop-05.html#outline",
    "title": "Deep Learning",
    "section": "Outline",
    "text": "Outline\n\nNeural Networks\nTransfer learning\nObject detection",
    "crumbs": [
      "Workshop",
      "Deep Learning"
    ]
  },
  {
    "objectID": "workshop-09.html",
    "href": "workshop-09.html",
    "title": "Large Language Models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Large Language Models"
    ]
  },
  {
    "objectID": "workshop-09.html#slides",
    "href": "workshop-09.html#slides",
    "title": "Large Language Models",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Workshop",
      "Large Language Models"
    ]
  },
  {
    "objectID": "workshop-09.html#outline",
    "href": "workshop-09.html#outline",
    "title": "Large Language Models",
    "section": "Outline",
    "text": "Outline\n\nClustering\nDimensionality Reduction\nRecommendation systems",
    "crumbs": [
      "Workshop",
      "Large Language Models"
    ]
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#introduction-to-machine-learning",
    "href": "slides/slides-02-ml-intro.html#introduction-to-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Introduction to Machine Learning",
    "text": "Introduction to Machine Learning\n\nMachine Learning uses computer programs to digest and accurately model data. After training on the data, a program can be used to extract hidden patterns, make predictions in new situations or generate novel content.\nThe program learns based on the features present in the data, which represent the information we have about each example."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#introduction-to-machine-learning-1",
    "href": "slides/slides-02-ml-intro.html#introduction-to-machine-learning-1",
    "title": "Introduction to Machine Learning",
    "section": "Introduction to Machine Learning",
    "text": "Introduction to Machine Learning"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#activity-1",
    "href": "slides/slides-02-ml-intro.html#activity-1",
    "title": "Introduction to Machine Learning",
    "section": "Activity 1",
    "text": "Activity 1\n\nWrite one (or several) problems in your research field where you think Machine Learning could be applied. Try to address the following questions:\n\nWhat goal are you trying to accomplish? What would an ideal solution to your problem look like?\nHow would a human solve this problem? What approaches are presently available and utilized?\nWhat kind of data is available to you, or might be collected? What features are present in the data?\n\nOne of the learning objectives of the workshop will be to determine whether your goal is best addressed using supervised machine learning, inferential statistics, unsupervised learning, deep learning, generative AI, or a non-ML solution."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#classification-vs.-regression",
    "href": "slides/slides-02-ml-intro.html#classification-vs.-regression",
    "title": "Introduction to Machine Learning",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#measuring-performance",
    "href": "slides/slides-02-ml-intro.html#measuring-performance",
    "title": "Introduction to Machine Learning",
    "section": "Measuring Performance",
    "text": "Measuring Performance\n\n\n\nPerformance on classification tasks can be measured based on the accuracy of the model’s predictions.\nPerformance on a regression task can be measured based on error. Mean squared error is one choice, but there are many others!"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#inference-vs.-prediction",
    "href": "slides/slides-02-ml-intro.html#inference-vs.-prediction",
    "title": "Introduction to Machine Learning",
    "section": "Inference vs. Prediction",
    "text": "Inference vs. Prediction\n\n\n\nInference is the use of a model to infer a relationship between features (independent variables) and targets (independent variables).\nPrediction is the use of a model to predict the target value for a new example not seen in training."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#what-outcome-do-we-care-about",
    "href": "slides/slides-02-ml-intro.html#what-outcome-do-we-care-about",
    "title": "Introduction to Machine Learning",
    "section": "What outcome do we care about?",
    "text": "What outcome do we care about?\n\n\n\nA researcher studying the impact of pollution on cancer risk is performing inference. They may not make perfect predictions (since the dataset is likely to be noisy) but good statistical inference could be extremely valuable.\nGmail’s spam filtering algorithm is performing prediction. We are not really trying to improve human understanding of what makes a message spam (often it is obvious), we just want a model that makes good predictions.\nOf course, these goals are related, so in many situations we may be interested in both."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#example-linear-regression",
    "href": "slides/slides-02-ml-intro.html#example-linear-regression",
    "title": "Introduction to Machine Learning",
    "section": "Example: Linear Regression",
    "text": "Example: Linear Regression\n\n\nIs this inference or prediction?"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#types-of-machine-learning",
    "href": "slides/slides-02-ml-intro.html#types-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nToday we will see two main types of machine learning, namely\n\nSupervised Learning, and\nUnsupervised Learning.\n\nWe will also discuss which problems each type might be best suited for."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#supervised-learning",
    "href": "slides/slides-02-ml-intro.html#supervised-learning",
    "title": "Introduction to Machine Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nHere the training data is comprised of a set of features, and each example comes with a corresponding target. The goal is to get a machine learning model to accurately predict the target based on the feature values.\nExamples could include spam filtering, face recognition or weather forecasting."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#unsupervised-learning",
    "href": "slides/slides-02-ml-intro.html#unsupervised-learning",
    "title": "Introduction to Machine Learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nIn unsupervised learning, there are no targets. The goal is instead to uncover underlying patterns. These can be used to provide a concise summary of the data, or group similar examples together.\nExamples could include customer segmentation, anomaly detection or online recommendation systems (think Netflix)."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#other-ml-types",
    "href": "slides/slides-02-ml-intro.html#other-ml-types",
    "title": "Introduction to Machine Learning",
    "section": "Other ML types",
    "text": "Other ML types\n\nSome other types of Machine Learning include self-supervised learning and reinforcement learning.\nSelf-supervised algorithms automatically learn to generate labels and transform unsupervised problems to supervised ones.\nReinforcement Leaning trains an agent using a system of rewards and penalties. The agent learns strategies to maximize reward. AlphaGo is a reinforcement learning agent that taught itself to play Go, and was able to beat the strongest human Go players."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#activity-2",
    "href": "slides/slides-02-ml-intro.html#activity-2",
    "title": "Introduction to Machine Learning",
    "section": "Activity 2",
    "text": "Activity 2\n \nReturn to the problems you identified in Activity 1. Try to decide if they involve performing inference or prediction.\nAlso suggest whether you think they are best approached with supervised or unsupervised learning. What aspects of the problem particularly suggest one approach over another?"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#a-simple-supervised-learning-model",
    "href": "slides/slides-02-ml-intro.html#a-simple-supervised-learning-model",
    "title": "Introduction to Machine Learning",
    "section": "A Simple Supervised Learning Model",
    "text": "A Simple Supervised Learning Model\n\nWe will use a simple machine learning model– a decision tree– to demonstrate some fundamental concepts in machine learning. Suppose we have the following dataset:\n\n\n\n\n\n\n\n\n\n\nml_experience\nclass_attendance\nlab1\nlab2\nlab3\nlab4\nquiz1\nquiz2\n\n\n\n\n0\n1\n1\n92\n93\n84\n91\n92\nA+\n\n\n1\n1\n0\n94\n90\n80\n83\n91\nnot A+\n\n\n2\n0\n0\n78\n85\n83\n80\n80\nnot A+\n\n\n3\n0\n1\n91\n94\n92\n91\n89\nA+\n\n\n4\n0\n1\n77\n83\n90\n92\n85\nA+\n\n\n\n\n\n\n\n  How would you go about predicting the Quiz 2 grade?"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#decision-trees",
    "href": "slides/slides-02-ml-intro.html#decision-trees",
    "title": "Introduction to Machine Learning",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nA decision tree iteratively splits the data by asking questions about feature values.\nThe algorithm tries to ask questions that best separate one class from another. It’s like a game of twenty questions!"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#a-decision-stump",
    "href": "slides/slides-02-ml-intro.html#a-decision-stump",
    "title": "Introduction to Machine Learning",
    "section": "A Decision Stump",
    "text": "A Decision Stump\n\n\n\n\n\n\n\n\n\n\n\nWe could start by splitting the data based on the students’ Lab 3 grades."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#iterating-the-procedure",
    "href": "slides/slides-02-ml-intro.html#iterating-the-procedure",
    "title": "Introduction to Machine Learning",
    "section": "Iterating the procedure",
    "text": "Iterating the procedure\n \n\n\n\n\n\n\n\n\n\nThen we further split each of the resulting nodes, again asking questions involving features in the dataset."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#building-a-decision-tree",
    "href": "slides/slides-02-ml-intro.html#building-a-decision-tree",
    "title": "Introduction to Machine Learning",
    "section": "Building a Decision Tree",
    "text": "Building a Decision Tree"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#decision-boundary",
    "href": "slides/slides-02-ml-intro.html#decision-boundary",
    "title": "Introduction to Machine Learning",
    "section": "Decision Boundary",
    "text": "Decision Boundary\n\nThe first two questions in our tree involved Lab 3 and Quiz 1 grades. We can make a plot involving these two features to better understand our tree."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#model-parameters",
    "href": "slides/slides-02-ml-intro.html#model-parameters",
    "title": "Introduction to Machine Learning",
    "section": "Model Parameters",
    "text": "Model Parameters\n\n\nDuring training, the model decides which feature to use to split at each node. It also decides which value of the feature to split at. This is the ‘learning’ phase, where the algorithm is trying different options and selecting the ‘best’ feature and value for splitting."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#hyperparameters",
    "href": "slides/slides-02-ml-intro.html#hyperparameters",
    "title": "Introduction to Machine Learning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nThe maximum depth of a decision tree (at most how many questions it asks) is a hyper-parameter of the model. We can build different trees to test which choice of hyper-parameter gives the best result.\nSome models may have a continuous range of options for a given hyper-parameter. This gives rise to a potentially infinite choice of “models” to test."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#trying-to-recognize-faces",
    "href": "slides/slides-02-ml-intro.html#trying-to-recognize-faces",
    "title": "Introduction to Machine Learning",
    "section": "Trying to Recognize Faces",
    "text": "Trying to Recognize Faces\n\nTo demonstrate some fundamental concepts in machine learning, we will attempt to biuld a decision tree that can recognize faces. Our data will be taken from the Olivetti Faces dataset, which is a collection of 400 images of faces.\nThe labels correspond to the forty individuals that are pictured, and the dataset contains 10 photos per individual. We will try to use a decision tree to correctly predict the individual for each photo."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#a-look-at-the-data",
    "href": "slides/slides-02-ml-intro.html#a-look-at-the-data",
    "title": "Introduction to Machine Learning",
    "section": "A Look at the Data",
    "text": "A Look at the Data\n\nEach photo is 64x64 pixels in grayscale. The images are represented by a row of pixel intensities showing how dark each individual pixel should be.\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n...\n4092\n4093\n4094\n4095\n\n\n\n\n0\n0.309917\n0.367769\n0.417355\n0.442149\n...\n0.148760\n0.152893\n0.161157\n0.157025\n\n\n1\n0.454545\n0.471074\n0.512397\n0.557851\n...\n0.152893\n0.152893\n0.152893\n0.152893\n\n\n2\n0.318182\n0.400826\n0.491736\n0.528926\n...\n0.144628\n0.140496\n0.148760\n0.152893\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n397\n0.500000\n0.533058\n0.607438\n0.628099\n...\n0.157025\n0.177686\n0.148760\n0.190083\n\n\n398\n0.214876\n0.219008\n0.219008\n0.223140\n...\n0.545455\n0.574380\n0.590909\n0.603306\n\n\n399\n0.516529\n0.462810\n0.280992\n0.252066\n...\n0.322314\n0.359504\n0.355372\n0.384298\n\n\n\n\n400 rows × 4096 columns\n\n\n\n\n\nThe dataset has 4096 features."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#a-decision-tree-classifier",
    "href": "slides/slides-02-ml-intro.html#a-decision-tree-classifier",
    "title": "Introduction to Machine Learning",
    "section": "A Decision Tree Classifier",
    "text": "A Decision Tree Classifier\n\nWe can build a decision tree classifier on the dataset of faces and see how it performs. For now we will train on a random subset of the data that contains 80% of the images (we’ll explain why later)\nLet’s see how accurate this model gets after training."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#a-decision-tree-classifier-1",
    "href": "slides/slides-02-ml-intro.html#a-decision-tree-classifier-1",
    "title": "Introduction to Machine Learning",
    "section": "A Decision Tree Classifier",
    "text": "A Decision Tree Classifier\n\n\n\n\nThe model classified 100.0% of training examples correctly by \n building a decision tree of depth 38\n\n\n\nThat’s very accurate indeed! Maybe decision trees are a really good way to detect and classify faces.\nRemember, we only trained on 80% of the data. Let’s see how our model performs on the remaining 20%"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#did-we-build-a-good-model",
    "href": "slides/slides-02-ml-intro.html#did-we-build-a-good-model",
    "title": "Introduction to Machine Learning",
    "section": "Did we build a good model?",
    "text": "Did we build a good model?\n\n\n\n\nThe model acheived an accuracy of 45.0% on new data\n\n\n\n…oops."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#whats-going-on",
    "href": "slides/slides-02-ml-intro.html#whats-going-on",
    "title": "Introduction to Machine Learning",
    "section": "What’s going on?",
    "text": "What’s going on?"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#practice-makes-perfect",
    "href": "slides/slides-02-ml-intro.html#practice-makes-perfect",
    "title": "Introduction to Machine Learning",
    "section": "Practice makes perfect",
    "text": "Practice makes perfect\n\nOur deep decision tree likely just memorized the dataset. After all, with a tree of depth 38, we could actually memorize up to 238 distinct examples!\nClearly this does not make for a good model. After all, we want a model that can recognize faces, even when they appear in new images."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#overfitting",
    "href": "slides/slides-02-ml-intro.html#overfitting",
    "title": "Introduction to Machine Learning",
    "section": "Overfitting",
    "text": "Overfitting\n\nOverfitting refers to a situation where the model learns noise from the training data, leading to a poor performance when deployed on new data.\nComplex models are prone to overfitting– we cannot just rely on training error to measure their performance. Simple models typically have similar train and test errors, but both will be high."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#section",
    "href": "slides/slides-02-ml-intro.html#section",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Thus we have our “fundamental tradeoff”: as we increase model complexity, the training error will reduce but the gap between training and test error will increase."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#scenario-1",
    "href": "slides/slides-02-ml-intro.html#scenario-1",
    "title": "Introduction to Machine Learning",
    "section": "Scenario 1",
    "text": "Scenario 1\n\nYour colleague is trying to build a machine learning model to detect cancers in medical imaging. They know about overfitting, so they separate their data into a training set and a test set.\nThey use 10 different types of machine learning models, and try 1000 different combinations of hyper-parameters for each. In every case, they only use the training set to train their model, and then note how the model performs by measure accuracy on the test set.\nThe best model achieves 99% accuracy on the test set. Your colleague tells you they have found a machine learning model that diagnoses cancer with 99% accuracy.\nDo you believe them?"
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#the-golden-rule-of-machine-learning",
    "href": "slides/slides-02-ml-intro.html#the-golden-rule-of-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "The Golden Rule of Machine Learning",
    "text": "The Golden Rule of Machine Learning\n\nBy using the same test set for each of the 10,000 models they tried, your colleague has violated the golden rule of machine learning.\nThe golden rule tells us that test data must not influence the model training in any way.\nEven though your colleague never directly trained on test data, they used test data multiple times to validate model performance. As a result, they are likely to have found good performance purely by accident."
  },
  {
    "objectID": "slides/slides-02-ml-intro.html#scenario-2",
    "href": "slides/slides-02-ml-intro.html#scenario-2",
    "title": "Introduction to Machine Learning",
    "section": "Scenario 2",
    "text": "Scenario 2\n\nYour colleague now separates their data into a training set, a validation set and a test set.\nThey again use 10 types of models and try 1000 combinations of hyper-parameters for each. They use the training set to train their model, and then note how the model performs by measure accuracy on the validation set.\nThe best model achieves 99% accuracy on the validation set, after which it is used on the test set. It achieves 99% accuracy again.\nDo you trust the outcome now?"
  }
]