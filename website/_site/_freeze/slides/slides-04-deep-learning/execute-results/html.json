{
  "hash": "477d4b7a045aa2bf535e2344deed7251",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deep Learning\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\njupyter: \n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n\n\n\n## Learning outcomes \n\\\nFrom this module, you will be able to \n\n- Explain the role of neural networks in machine learning, including their advantages and disadvantages.\n- Discuss why traditional methods are less effective for image data.\n- Gain a high-level understanding of transfer learning.\n- Differentiate between image classification and object detection.\n\n## Introduction to neural networks\n\\\n\n- Neural networks are very popular these days under the name deep learning.\n- Neural networks apply a sequence of transformations on your input data.\n- They can be viewed a generalization of linear models where we apply a series of transformations.\n- Here is graphical representation of a logistic regression model.\n- We have 4 features: x[0], x[1], x[2], x[3]\n\n::: {#f7f35d5b .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](slides-04-deep-learning_files/figure-revealjs/cell-3-output-1.svg){}\n:::\n:::\n\n\n## Adding a layer of transformations \n\\\n\n- Below we are adding one \"layer\" of transformations in between features and the target. \n- We are repeating the the process of computing the weighted sum multiple times.  \n- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. \n\n::: {#6f3bc06c .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](slides-04-deep-learning_files/figure-revealjs/cell-4-output-1.svg){}\n:::\n:::\n\n\n## One more layer of transformations \n\\\n\n- Now we are adding one more layer of transformations. \n\n::: {#0649bda8 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](slides-04-deep-learning_files/figure-revealjs/cell-5-output-1.svg){}\n:::\n:::\n\n\n## Neural networks \n\\\n\n- A neural network is a model that's sort of like its own pipeline\n  - It involves a series of transformations (\"layers\") internally. \n  - The output is the prediction.\n\n- With a neural net, you specify the number of features after each transformation.\n  - In the above, it goes from 4 to 3 to 3 to 1.\n\n- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. \n- Neural network = neural net\n- Deep learning ~ using neural networks\n\n## Why neural networks?\n\\\n\n- They can learn very complex functions.\n  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.\n  - More layers / bigger layers --> more complex model.\n  - You can generally get a model that will not underfit. \n\n- The work really well for structured data:\n  - 1D sequence, e.g. timeseries, language\n  - 2D image\n  - 3D image or video\n- They've had some incredible successes in the last 10 years.\n- Transfer learning (coming later today) is really useful.  \n\n## Why not neural networks?\n\\\n\n- Often they require a lot of data.\n- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n- They have huge numbers of hyperparameters are a huge pain to tune.\n  - Think of each layer having hyperparameters, plus some overall hyperparameters.\n  - Being slow compounds this problem.\n- They are not interpretable.\n- I don't recommend training them on your own without further training\n\n  - Good news\n    - You don't have to train your models from scratch in order to use them.\n    - I'll show you some ways to use neural networks without training them yourselves. \n\n## Deep learning software\n\\\n\nThe current big players are:\n\n1. [PyTorch](http://pytorch.org)\n2. [TensorFlow](https://www.tensorflow.org)\n\nBoth are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n\n<br><br>\n\n## Introduction to computer vision\n\\\n\n- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. It has many tasks of interest:\n    - image classification: is this a cat or a dog?\n    - object localization: where is the cat in this image?\n    - object detection: What are the various objects in the image? \n    - instance segmentation: What are the shapes of these various objects in the image? \n    - and much more...\n\n![](img/vision-apps.jpeg)\n\n<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> \n\nIn the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.\n\n## Image classification\n\\\n\nHave you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\nThis can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.\n\n## Image classification\n\\\n\nImage classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.\n\n![](img/cat_variation.png)\n<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->\n\n## Convolutional Neural Networks\n\\\n\n- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. \n- [ImageNet Classification with Deep Convolutional\nNeural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n- Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. \n\n## Pre-trained models\n\\\n\n- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\n- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.\n- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\n- It refers to using a model already trained on one task as a starting point for learning to perform another task\n\n## Pre-trained models\n\\\n\n- There are many famous deep learning architectures out there that have been very successful across a wide range of problems, e.g.: [AlexNet](https://arxiv.org/abs/1404.5997), [VGG](https://arxiv.org/abs/1409.1556), [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/abs/1512.00567), [MobileNet](https://arxiv.org/abs/1801.04381), etc.\n\n- Many of these models have been pre-trained on famous datasets like ImageNet [[1](https://www.image-net.org/index.php), [2](https://en.wikipedia.org/wiki/ImageNet)]\n\n## ImageNet\n\\\n\n- [ImageNet](http://www.image-net.org/) is an image dataset that became a very popular benchmark in the field ~12 years ago. \n- Currently contains ~14 million labelled images with ~21,841 categories  \n- There are various versions with different number of images and classes\n    - ILSVRC, a popular annual competition in computer vision, uses a smaller subset of ImageNet. This subset consists of about 1.2 million training images, 50,000 validation images, and 150,000 testing images across 1,000 categories. \n- [Wikipedia article](https://en.wikipedia.org/wiki/ImageNet) on ImageNet\n\n## ImageNet classes \n\\\n\n- Here are some example classes. \n\n::: {#fb54cc07 .cell execution_count=5}\n``` {.python .cell-code}\nwith open(\"data/imagenet_classes.txt\") as f:\n    classes = [line.strip() for line in f.readlines()]\nclasses[100:110]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n['black swan, Cygnus atratus',\n 'tusker',\n 'echidna, spiny anteater, anteater',\n 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n 'wallaby, brush kangaroo',\n 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n 'wombat',\n 'jellyfish',\n 'sea anemone, anemone',\n 'brain coral']\n```\n:::\n:::\n\n\n## Transfer learning \n\\\n\n- The idea of transfer learning is instead of developing a machine learning model from scratch, you use these available pre-trained models for your tasks either directly or by fine tuning them. \n- There are three common ways to use transfer learning in computer vision \n    1. Using pre-trained models out-of-the-box \n    2. Using pre-trained models as feature extractor and training your own model with these features\n    2. Starting with weights of pre-trained models and fine-tuning the weights for your task. \n- We will explore the first two approaches.     \n\n## Using pre-trained models out-of-the-box \n\\\n\n![](img/cnn-ex.png)\n\n<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->\n\n- Let's first try one of these models and apply it to our own problem right out of the box. \n\n\n## Using pre-trained models out-of-the-box \n\\\n\n- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)\n- We used a pre-trained model vgg16 which is trained on the ImageNet data. \n- We preprocess the given image. \n- We get prediction from this pre-trained model on a given image along with prediction probabilities.  \n- For a given image, this model will spit out one of the 1000 classes from ImageNet. \n\n## Using pre-trained models out-of-the-box {.scrollable}\n\n- Let's predict labels with associated probabilities for unseen images\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#80c3d924 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-7-output-1.png){width=434 height=325}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\n                     tiger cat              0.353\n              tabby, tabby cat              0.207\n               lynx, catamount              0.050\nPembroke, Pembroke Welsh corgi              0.046\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-7-output-3.png){width=324 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.983\n                  leopard, Panthera pardus              0.012\njaguar, panther, Panthera onca, Felis onca              0.004\n       snow leopard, ounce, Panthera uncia              0.001\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-7-output-5.png){width=309 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                   Class  Probability score\n                                 macaque              0.714\npatas, hussar monkey, Erythrocebus patas              0.122\n      proboscis monkey, Nasalis larvatus              0.098\n                   guenon, guenon monkey              0.017\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-7-output-7.png){width=299 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Class  Probability score\nWalker hound, Walker foxhound              0.580\n             English foxhound              0.091\n                  EntleBucher              0.080\n                       beagle              0.065\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n\n## Using pre-trained models out-of-the-box \n\\\n\n- We got these predictions without \"doing the ML ourselves\".\n- We are using **pre-trained** `vgg16` model which is available in `torchvision`.\n  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n- Many of these models have been pre-trained on famous datasets like **ImageNet**. \n- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. \n\n## Using pre-trained models out-of-the-box {.smaller}\n\\\n\n- Let's try some images which are unlikely to be there in ImageNet. \n- It's not doing very well here because ImageNet doesn't have proper classes for these images.\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#5b310585 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-1.png){width=549 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         Class  Probability score\ncucumber, cuke              0.146\n         plate              0.117\n     guacamole              0.099\n  Granny Smith              0.091\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-3.png){width=330 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Class  Probability score\nchocolate sauce, chocolate syrup              0.609\n                        espresso              0.039\n             ice cream, icecream              0.037\n                     face powder              0.030\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-5.png){width=702 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Class  Probability score\n                                        fig              0.637\n                                pomegranate              0.193\ngrocery store, grocery, food market, market              0.041\n                                      crate              0.023\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-7.png){width=611 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                               Class  Probability score\n                                         toilet seat              0.171\n                                          safety pin              0.060\nbannister, banister, balustrade, balusters, handrail              0.039\n                                              bubble              0.035\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-9.png){width=572 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                    Class  Probability score\nrobin, American robin, Turdus migratorius              0.203\n                                  jacamar              0.156\n                                   bulbul              0.122\n      brambling, Fringilla montifringilla              0.103\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-11.png){width=609 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\ngoldfinch, Carduelis carduelis              0.585\n                     bee eater              0.219\n                        toucan              0.038\n                      hornbill              0.026\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-13.png){width=428 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Class  Probability score\n                   vase              0.078\n                thimble              0.074\n             plate rack              0.049\nsaltshaker, salt shaker              0.047\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-15.png){width=331 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      Class  Probability score\n           pizza, pizza pie              0.998\nfrying pan, frypan, skillet              0.001\n                     potpie              0.000\n                French loaf              0.000\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-8-output-17.png){width=607 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n              Class  Probability score\n     patio, terrace              0.213\n           fountain              0.164\nlakeside, lakeshore              0.097\n            sundial              0.088\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n## Using pre-trained models out-of-the-box\n\\\n\n- Here we are using pre-trained models out-of-the-box. \n- Can we use pre-trained models for our own classification problem with our classes? \n- Yes!! We have two options here:\n    1. Add some extra layers to the pre-trained network to suit our particular task\n    2. Pass training data through the network and save the output to use as features for training some other model\n\n\n## Using pre-trained models to extract features \n\\\n\n- Let's use pre-trained models to extract features.\n- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. \n- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. \n- You can think of each layer a transformer applying some transformations on the input received to that later. \n\n![](img/cnn-ex.png)\n\n\n## Using pre-trained models to extract features \n\\\n\n- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. \n- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  \n- Let's try this out. \n- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. \n\n## Using pre-trained models to extract features \n\\\n\nLet's look at some sample images in the dataset. \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#1cd4a069 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-9-output-1.png){width=611 height=631}\n:::\n:::\n\n\n:::\n\n## Dataset statistics\n\\\n\nHere is the stat of our toy dataset. \n\n::: {#87f9dbba .cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses: ['beet_salad', 'chocolate_cake', 'edamame', 'french_fries', 'pizza', 'spring_rolls', 'sushi']\nClass count: 40, 38, 40\nSamples: 283\nFirst sample: ('data/food/train/beet_salad/104294.jpg', 0)\n```\n:::\n:::\n\n\n## Using pre-trained models to extract features \n\\\n\n- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  \n\n\n\n## Shape of the feature vector\n\\\n\n- Now we have extracted feature vectors for all examples. What's the shape of these features?\n\n::: {#312ebed9 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\ntorch.Size([283, 1024])\n```\n:::\n:::\n\n\n- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  \n\n![](img/densenet-architecture.png)\n\n[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)\n\n## A feature vector given by densenet \n\\ \n\n- Let's examine the feature vectors. \n\n::: {#bdccc79d .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1014</th>\n      <th>1015</th>\n      <th>1016</th>\n      <th>1017</th>\n      <th>1018</th>\n      <th>1019</th>\n      <th>1020</th>\n      <th>1021</th>\n      <th>1022</th>\n      <th>1023</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000397</td>\n      <td>0.006987</td>\n      <td>0.002080</td>\n      <td>0.001633</td>\n      <td>0.164544</td>\n      <td>1.284627</td>\n      <td>0.000326</td>\n      <td>0.003747</td>\n      <td>0.019033</td>\n      <td>0.000411</td>\n      <td>...</td>\n      <td>2.017400</td>\n      <td>0.265346</td>\n      <td>4.620863</td>\n      <td>0.285282</td>\n      <td>0.526915</td>\n      <td>0.428759</td>\n      <td>2.960554</td>\n      <td>1.577721</td>\n      <td>2.150136</td>\n      <td>0.735927</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000639</td>\n      <td>0.004926</td>\n      <td>0.000985</td>\n      <td>0.001324</td>\n      <td>0.145076</td>\n      <td>0.558028</td>\n      <td>0.000775</td>\n      <td>0.002603</td>\n      <td>0.181858</td>\n      <td>0.000249</td>\n      <td>...</td>\n      <td>0.158792</td>\n      <td>0.380575</td>\n      <td>1.309545</td>\n      <td>0.009790</td>\n      <td>0.007745</td>\n      <td>1.773695</td>\n      <td>0.298153</td>\n      <td>0.697784</td>\n      <td>0.462329</td>\n      <td>1.448314</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000109</td>\n      <td>0.006808</td>\n      <td>0.003910</td>\n      <td>0.002917</td>\n      <td>0.137208</td>\n      <td>0.702670</td>\n      <td>0.000546</td>\n      <td>0.003634</td>\n      <td>0.386037</td>\n      <td>0.000179</td>\n      <td>...</td>\n      <td>1.730925</td>\n      <td>0.139263</td>\n      <td>0.384295</td>\n      <td>1.754404</td>\n      <td>0.267863</td>\n      <td>0.480649</td>\n      <td>0.591291</td>\n      <td>3.880836</td>\n      <td>1.436774</td>\n      <td>0.856637</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000350</td>\n      <td>0.002965</td>\n      <td>0.001391</td>\n      <td>0.000091</td>\n      <td>0.119358</td>\n      <td>0.032185</td>\n      <td>0.000413</td>\n      <td>0.004407</td>\n      <td>0.000000</td>\n      <td>0.000120</td>\n      <td>...</td>\n      <td>0.866084</td>\n      <td>0.118614</td>\n      <td>0.404703</td>\n      <td>0.139143</td>\n      <td>0.588674</td>\n      <td>0.326834</td>\n      <td>0.277743</td>\n      <td>2.369125</td>\n      <td>0.295955</td>\n      <td>0.206888</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000350</td>\n      <td>0.003532</td>\n      <td>0.002254</td>\n      <td>0.002841</td>\n      <td>0.102675</td>\n      <td>0.382482</td>\n      <td>0.000436</td>\n      <td>0.004212</td>\n      <td>0.816037</td>\n      <td>0.000554</td>\n      <td>...</td>\n      <td>0.010685</td>\n      <td>1.147245</td>\n      <td>1.892539</td>\n      <td>1.929401</td>\n      <td>0.694761</td>\n      <td>0.744420</td>\n      <td>1.705220</td>\n      <td>1.241349</td>\n      <td>2.351264</td>\n      <td>0.054507</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1024 columns</p>\n</div>\n```\n:::\n:::\n\n\n- The features are hard to interpret but they have some important information about the images which can be useful for classification.  \n\n## Logistic regression with the extracted features \n\\\n\n- Let's try out logistic regression on these extracted features. \n\n::: {#4c2699f7 .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score:  1.0\n```\n:::\n:::\n\n\n::: {#5860f24d .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation score:  0.855072463768116\n```\n:::\n:::\n\n\n- This is great accuracy for so little data and little effort!!!\n\n\n## Sample predictions\n\\\n\nLet's examine some sample predictions on the validation set.  \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#311db887 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-16-output-1.png){width=1151 height=1901}\n:::\n:::\n\n\n:::\n\n\n## Object detection using [YOLO](https://docs.ultralytics.com/)\n\\\n\n- Another useful task and tool to know is object detection using YOLO model. \n- Let's identify objects in a sample image using a pretrained model called YOLO5\n\n::: {#a100647f .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\nrequirements: Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\nRequirement already satisfied: gitpython>=3.1.30 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.1.43)\nRequirement already satisfied: pillow>=10.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (10.4.0)\nRequirement already satisfied: requests>=2.32.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.32.3)\nRequirement already satisfied: setuptools>=70.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (72.2.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gitpython>=3.1.30) (4.0.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (1.26.6)\nRequirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (3.2)\nRequirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (2021.5.30)\nRequirement already satisfied: smmap<5,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (4.0.0)\n\nrequirements: AutoUpdate success ✅ 3.0s, installed 4 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0', 'setuptools>=70.0.0']\nrequirements: ⚠️ Restart runtime or rerun command for updates to take effect\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-04-deep-learning_files/figure-revealjs/cell-17-output-2.png){}\n:::\n:::\n\n\n## Summary \n\\\n\n- Neural networks are a flexible class of models.\n  - They are hard to train\n  - They are particular powerful for structured input like images, videos, audio, etc.\n- The good news is we can use pre-trained neural networks.\n  - This saves us a huge amount of time/cost/effort/resources.\n  - We can use these pre-trained networks directly or use them as feature transformers. \n\n",
    "supporting": [
      "slides-04-deep-learning_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}