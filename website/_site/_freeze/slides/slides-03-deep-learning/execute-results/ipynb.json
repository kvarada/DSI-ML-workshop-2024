{
  "hash": "44a61ce3454043ea109f0e4e0038a0f8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deep Learning\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\n    ipynb: default\njupyter: \n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n\n::: {#4ca0c7b3 .cell execution_count=1}\n``` {.python .cell-code}\nimport mglearn\nimport json\nimport numpy as np\nimport pandas as pd\nimport sys, os\nsys.path.append(os.path.join(os.path.abspath(\".\"), \"code\"))\nfrom deep_learning_code import *\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom torchvision import datasets, models, transforms, utils\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.image as mpimg\n%matplotlib inline\n```\n:::\n\n\n## Learning outcomes \n\\\nFrom this module, you will be able to \n\n- Explain the role of neural networks in machine learning, including their advantages and disadvantages.\n- Discuss why traditional methods are less effective for image data.\n- Gain a high-level understanding of transfer learning.\n- Differentiate between image classification and object detection.\n\n## Introduction to neural networks\n\\\n\n- Neural networks are very popular these days under the name deep learning.\n- Neural networks apply a sequence of transformations on your input data.\n- They can be viewed a generalization of linear models where we apply a series of transformations.\n- Here is graphical representation of a logistic regression model.\n- We have 4 features: x[0], x[1], x[2], x[3]\n\n::: {#91c0bed4 .cell execution_count=2}\n``` {.python .cell-code}\nimport mglearn\n\nmglearn.plots.plot_logistic_regression_graph()\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n![](slides-03-deep-learning_files/figure-ipynb/cell-3-output-1.svg){}\n:::\n:::\n\n\n## Adding a layer of transformations \n\\\n\n- Below we are adding one \"layer\" of transformations in between features and the target. \n- We are repeating the the process of computing the weighted sum multiple times.  \n- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. \n\n::: {#086d7f2f .cell execution_count=3}\n``` {.python .cell-code}\nmglearn.plots.plot_single_hidden_layer_graph()\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n![](slides-03-deep-learning_files/figure-ipynb/cell-4-output-1.svg){}\n:::\n:::\n\n\n## One more layer of transformations \n\\\n\n- Now we are adding one more layer of transformations. \n\n::: {#c6c622d0 .cell execution_count=4}\n``` {.python .cell-code}\nmglearn.plots.plot_two_hidden_layer_graph()\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n![](slides-03-deep-learning_files/figure-ipynb/cell-5-output-1.svg){}\n:::\n:::\n\n\n## Neural networks \n\\\n\n- A neural network is a model that's sort of like its own pipeline\n  - It involves a series of transformations (\"layers\") internally. \n  - The output is the prediction.\n\n- With a neural net, you specify the number of features after each transformation.\n  - In the above, it goes from 4 to 3 to 3 to 1.\n\n- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. \n- Neural network = neural net\n- Deep learning ~ using neural networks\n\n## Why neural networks?\n\\\n\n- They can learn very complex functions.\n  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.\n  - More layers / bigger layers --> more complex model.\n  - You can generally get a model that will not underfit. \n\n- The work really well for structured data:\n  - 1D sequence, e.g. timeseries, language\n  - 2D image\n  - 3D image or video\n- They've had some incredible successes in the last 10 years.\n- Transfer learning (coming later today) is really useful.  \n\n## Why not neural networks?\n\\\n\n- Often they require a lot of data.\n- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n- They have huge numbers of hyperparameters are a huge pain to tune.\n  - Think of each layer having hyperparameters, plus some overall hyperparameters.\n  - Being slow compounds this problem.\n- They are not interpretable.\n- I don't recommend training them on your own without further training\n\n  - Good news\n    - You don't have to train your models from scratch in order to use them.\n    - I'll show you some ways to use neural networks without training them yourselves. \n\n## Deep learning software\n\\\n\nThe current big players are:\n\n1. [PyTorch](http://pytorch.org)\n2. [TensorFlow](https://www.tensorflow.org)\n\nBoth are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n\n<br><br>\n\n## Introduction to computer vision\n\\\n\n- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. It has many tasks of interest:\n    - image classification: is this a cat or a dog?\n    - object localization: where is the cat in this image?\n    - object detection: What are the various objects in the image? \n    - instance segmentation: What are the shapes of these various objects in the image? \n    - and much more...\n\n![](img/vision-apps.jpeg)\n\n<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> \n\nIn the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.\n\n## Image classification\n\\\n\nHave you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\nThis can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.\n\n## Image classification\n\\\n\nImage classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.\n\n![](img/cat_variation.png)\n<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->\n\n## Convolutional Neural Networks\n\\\n\n- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. \n- [ImageNet Classification with Deep Convolutional\nNeural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n- Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. \n\n## Pre-trained models\n\\\n\n- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\n- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.\n- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\n- It refers to using a model already trained on one task as a starting point for learning to perform another task\n\n## Pre-trained models\n\\\n\n- There are many famous deep learning architectures out there that have been very successful across a wide range of problems, e.g.: [AlexNet](https://arxiv.org/abs/1404.5997), [VGG](https://arxiv.org/abs/1409.1556), [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/abs/1512.00567), [MobileNet](https://arxiv.org/abs/1801.04381), etc.\n\n- Many of these models have been pre-trained on famous datasets like ImageNet [[1](https://www.image-net.org/index.php), [2](https://en.wikipedia.org/wiki/ImageNet)]\n\n## ImageNet\n\\\n\n- [ImageNet](http://www.image-net.org/) is an image dataset that became a very popular benchmark in the field ~12 years ago. \n- Currently contains ~14 million labelled images with ~21,841 categories  \n- There are various versions with different number of images and classes\n    - ILSVRC, a popular annual competition in computer vision, uses a smaller subset of ImageNet. This subset consists of about 1.2 million training images, 50,000 validation images, and 150,000 testing images across 1,000 categories. \n- [Wikipedia article](https://en.wikipedia.org/wiki/ImageNet) on ImageNet\n\n## ImageNet classes \n\\\n\n- Here are some example classes. \n\n::: {#991ca4a8 .cell execution_count=5}\n``` {.python .cell-code}\nwith open(\"data/imagenet_classes.txt\") as f:\n    classes = [line.strip() for line in f.readlines()]\nclasses[100:110]\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n['black swan, Cygnus atratus',\n 'tusker',\n 'echidna, spiny anteater, anteater',\n 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n 'wallaby, brush kangaroo',\n 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n 'wombat',\n 'jellyfish',\n 'sea anemone, anemone',\n 'brain coral']\n```\n:::\n:::\n\n\n## Transfer learning \n\\\n\n- The idea of transfer learning is instead of developing a machine learning model from scratch, you use these available pre-trained models for your tasks either directly or by fine tuning them. \n- There are three common ways to use transfer learning in computer vision \n    1. Using pre-trained models out-of-the-box \n    2. Using pre-trained models as feature extractor and training your own model with these features\n    2. Starting with weights of pre-trained models and fine-tuning the weights for your task. \n- We will explore the first two approaches.     \n\n## Using pre-trained models out-of-the-box \n\\\n\n![](img/cnn-ex.png)\n\n<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->\n\n- Let's first try one of these models and apply it to our own problem right out of the box. \n\n\n## Using pre-trained models out-of-the-box \n\\\n\n- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)\n- We used a pre-trained model vgg16 which is trained on the ImageNet data. \n- We preprocess the given image. \n- We get prediction from this pre-trained model on a given image along with prediction probabilities.  \n- For a given image, this model will spit out one of the 1000 classes from ImageNet. \n\n## Using pre-trained models out-of-the-box {.scrollable}\n\n- Let's predict labels with associated probabilities for unseen images\n\n::: {#0ebce653 .cell execution_count=6}\n``` {.python .cell-code}\nimport glob\nimport matplotlib.pyplot as plt\nimages = glob.glob(\"data/test_images/*.*\")\nplt.figure(figsize=(5, 5));\nfor image in images:\n    img = Image.open(image)\n    img.load()\n    \n    plt.imshow(img)\n    plt.show()\n    df = classify_image(img)\n    print(df.to_string(index=False))\n    print(\"--------------------------------------------------------------\")\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-7-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\n                     tiger cat              0.353\n              tabby, tabby cat              0.207\n               lynx, catamount              0.050\nPembroke, Pembroke Welsh corgi              0.046\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-7-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.983\n                  leopard, Panthera pardus              0.012\njaguar, panther, Panthera onca, Felis onca              0.004\n       snow leopard, ounce, Panthera uncia              0.001\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-7-output-5.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                   Class  Probability score\n                                 macaque              0.714\npatas, hussar monkey, Erythrocebus patas              0.122\n      proboscis monkey, Nasalis larvatus              0.098\n                   guenon, guenon monkey              0.017\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-7-output-7.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Class  Probability score\nWalker hound, Walker foxhound              0.580\n             English foxhound              0.091\n                  EntleBucher              0.080\n                       beagle              0.065\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n## Using pre-trained models out-of-the-box \n\\\n\n- We got these predictions without \"doing the ML ourselves\".\n- We are using **pre-trained** `vgg16` model which is available in `torchvision`.\n  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n- Many of these models have been pre-trained on famous datasets like **ImageNet**. \n- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. \n\n## Using pre-trained models out-of-the-box {.smaller}\n\\\n\n- Let's try some images which are unlikely to be there in ImageNet. \n- It's not doing very well here because ImageNet doesn't have proper classes for these images.\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#a4e459d6 .cell execution_count=7}\n``` {.python .cell-code}\n# Predict labels with associated probabilities for unseen images\nimages = glob.glob(\"data/random_img/*.*\")\nfor image in images:\n    img = Image.open(image)\n    img.load()\n    plt.imshow(img)\n    plt.show()\n    df = classify_image(img)    \n    print(df.to_string(index=False))\n    print(\"--------------------------------------------------------------\")\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         Class  Probability score\ncucumber, cuke              0.146\n         plate              0.117\n     guacamole              0.099\n  Granny Smith              0.091\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Class  Probability score\nchocolate sauce, chocolate syrup              0.609\n                        espresso              0.039\n             ice cream, icecream              0.037\n                     face powder              0.030\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-5.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Class  Probability score\n                                        fig              0.637\n                                pomegranate              0.193\ngrocery store, grocery, food market, market              0.041\n                                      crate              0.023\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-7.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                               Class  Probability score\n                                         toilet seat              0.171\n                                          safety pin              0.060\nbannister, banister, balustrade, balusters, handrail              0.039\n                                              bubble              0.035\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-9.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                    Class  Probability score\nrobin, American robin, Turdus migratorius              0.203\n                                  jacamar              0.156\n                                   bulbul              0.122\n      brambling, Fringilla montifringilla              0.103\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-11.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\ngoldfinch, Carduelis carduelis              0.585\n                     bee eater              0.219\n                        toucan              0.038\n                      hornbill              0.026\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-13.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Class  Probability score\n                   vase              0.078\n                thimble              0.074\n             plate rack              0.049\nsaltshaker, salt shaker              0.047\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-15.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      Class  Probability score\n           pizza, pizza pie              0.998\nfrying pan, frypan, skillet              0.001\n                     potpie              0.000\n                French loaf              0.000\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-8-output-17.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n              Class  Probability score\n     patio, terrace              0.213\n           fountain              0.164\nlakeside, lakeshore              0.097\n            sundial              0.088\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n:::\n\n## Using pre-trained models out-of-the-box\n\\\n\n- Here we are using pre-trained models out-of-the-box. \n- Can we use pre-trained models for our own classification problem with our classes? \n- Yes!! We have two options here:\n    1. Add some extra layers to the pre-trained network to suit our particular task\n    2. Pass training data through the network and save the output to use as features for training some other model\n\n\n## Using pre-trained models to extract features \n\\\n\n- Let's use pre-trained models to extract features.\n- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. \n- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. \n- You can think of each layer a transformer applying some transformations on the input received to that later. \n\n![](img/cnn-ex.png)\n\n\n## Using pre-trained models to extract features \n\\\n\n- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. \n- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  \n- Let's try this out. \n- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. \n\n## Using pre-trained models to extract features \n\\\n\nLet's look at some sample images in the dataset. \n\n::: {#dbb39701 .cell execution_count=8}\n``` {.python .cell-code}\n    data_dir = 'data/food/'\n    image_datasets, dataloaders = read_data(data_dir)\n    dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"valid\"]}\n    class_names = image_datasets[\"train\"].classes\n    inputs, classes = next(iter(dataloaders[\"valid\"]))\n    plt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample valid Images\")\n    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-9-output-1.png){}\n:::\n:::\n\n\n## Dataset statistics\n\\\n\nHere is the stat of our toy dataset. \n\n::: {#5d51fb90 .cell execution_count=9}\n``` {.python .cell-code}\n    print(f\"Classes: {image_datasets['train'].classes}\")\n    print(f\"Class count: {image_datasets['train'].targets.count(0)}, {image_datasets['train'].targets.count(1)}, {image_datasets['train'].targets.count(2)}\")\n    print(f\"Samples:\", len(image_datasets[\"train\"]))\n    print(f\"First sample: {image_datasets['train'].samples[0]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses: ['beet_salad', 'chocolate_cake', 'edamame', 'french_fries', 'pizza', 'spring_rolls', 'sushi']\nClass count: 40, 38, 40\nSamples: 283\nFirst sample: ('data/food/train/beet_salad/104294.jpg', 0)\n```\n:::\n:::\n\n\n## Using pre-trained models to extract features \n\\\n\n- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  \n\n::: {#efdb9701 .cell execution_count=10}\n``` {.python .cell-code}\ndensenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\ndensenet.classifier = nn.Identity()  # remove that last \"classification\" layer\nZ_train, y_train, Z_valid, y_valid = get_features(\n    densenet, dataloaders[\"train\"], dataloaders[\"valid\"]\n)\n```\n:::\n\n\n## Shape of the feature vector\n\\\n\n- Now we have extracted feature vectors for all examples. What's the shape of these features?\n\n::: {#3fc0b9f7 .cell execution_count=11}\n``` {.python .cell-code}\nZ_train.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\ntorch.Size([283, 1024])\n```\n:::\n:::\n\n\n- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  \n\n![](img/densenet-architecture.png)\n\n[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)\n\n## A feature vector given by densenet \n\\ \n\n- Let's examine the feature vectors. \n\n::: {#be090c0e .cell execution_count=12}\n``` {.python .cell-code}\npd.DataFrame(Z_train).head()\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>...</th>\n      <th>1019</th>\n      <th>1020</th>\n      <th>1021</th>\n      <th>1022</th>\n      <th>1023</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000243</td>\n      <td>0.005287</td>\n      <td>0.000781</td>\n      <td>0.000698</td>\n      <td>0.130936</td>\n      <td>...</td>\n      <td>0.353761</td>\n      <td>0.748432</td>\n      <td>4.806498</td>\n      <td>0.569595</td>\n      <td>1.140131</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000469</td>\n      <td>0.003634</td>\n      <td>0.002736</td>\n      <td>0.002155</td>\n      <td>0.114086</td>\n      <td>...</td>\n      <td>0.954528</td>\n      <td>0.539258</td>\n      <td>1.892998</td>\n      <td>2.541887</td>\n      <td>1.368175</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000774</td>\n      <td>0.003644</td>\n      <td>0.005294</td>\n      <td>0.002800</td>\n      <td>0.120463</td>\n      <td>...</td>\n      <td>0.076066</td>\n      <td>0.066211</td>\n      <td>1.731157</td>\n      <td>0.569258</td>\n      <td>0.085943</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000243</td>\n      <td>0.006314</td>\n      <td>0.004296</td>\n      <td>0.003802</td>\n      <td>0.045087</td>\n      <td>...</td>\n      <td>3.257711</td>\n      <td>0.004062</td>\n      <td>0.783686</td>\n      <td>1.095625</td>\n      <td>0.818525</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000284</td>\n      <td>0.002082</td>\n      <td>0.002930</td>\n      <td>0.005098</td>\n      <td>0.126708</td>\n      <td>...</td>\n      <td>2.622521</td>\n      <td>1.264073</td>\n      <td>0.986093</td>\n      <td>0.391501</td>\n      <td>1.632058</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 1024 columns</p>\n</div>\n```\n:::\n:::\n\n\n- The features are hard to interpret but they have some important information about the images which can be useful for classification.  \n\n## Logistic regression with the extracted features \n\\\n\n- Let's try out logistic regression on these extracted features. \n\n::: {#2f761ad2 .cell execution_count=13}\n``` {.python .cell-code}\npipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\npipe.fit(Z_train, y_train)\nprint(\"Training score: \", pipe.score(Z_train, y_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score:  1.0\n```\n:::\n:::\n\n\n::: {#b9f8383a .cell execution_count=14}\n``` {.python .cell-code}\npipe.score(Z_valid, y_valid)\nprint(\"Validation score: \", pipe.score(Z_valid, y_valid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation score:  0.8405797101449275\n```\n:::\n:::\n\n\n- This is great accuracy for so little data and little effort!!!\n\n\n## Sample predictions\n\\\n\nLet's examine some sample predictions on the validation set.  \n\n::: {#c92e0a66 .cell execution_count=15}\n``` {.python .cell-code}\n# Show predictions for 25 images in the validation set (5 rows of 5 images)\nshow_predictions(pipe, Z_valid, y_valid, dataloaders['valid'], class_names, num_images=40)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-16-output-1.png){}\n:::\n:::\n\n\n## Object detection using [YOLO](https://docs.ultralytics.com/)\n\\\n\n- Another useful task and tool to know is object detection using YOLO model. \n- Let's identify objects in a sample image using a pretrained model called YOLO5\n\n::: {#d31106c3 .cell execution_count=16}\n``` {.python .cell-code}\nimport torch\nimport cv2\nfrom matplotlib import pyplot as plt\n\n# Load YOLOv5 model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n\n# Load image\nimg = 'data/yolo_test/3356700488_183566145b.jpg'\n\n# Perform inference\nresults = model(img)\n\n# Print results\nresults.print()  # prints results to console\n\n# Show results\nresults.show()  # displays image with bounding boxes\n\n# Save results\n# results.save()  # save image with detections to 'runs/detect/exp'\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrequirements: Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing cache found in /Users/kvarada/.cache/torch/hub/ultralytics_yolov5_master\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: gitpython>=3.1.30 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.1.43)\nRequirement already satisfied: pillow>=10.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (10.4.0)\nRequirement already satisfied: requests>=2.32.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.32.3)\nRequirement already satisfied: setuptools>=70.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (72.2.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gitpython>=3.1.30) (4.0.7)\nRequirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (2021.5.30)\nRequirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (3.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests>=2.32.0) (1.26.6)\nRequirement already satisfied: smmap<5,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (4.0.0)\n\nrequirements: AutoUpdate success ‚úÖ 3.1s, installed 4 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0', 'setuptools>=70.0.0']\nrequirements: ‚ö†Ô∏è Restart runtime or rerun command for updates to take effect\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWARNING: You are using pip version 22.0.4; however, version 24.2 is available.\nYou should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\nYOLOv5 üöÄ 2024-8-17 Python-3.10.0 torch-2.0.0.post2 CPU\n\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\nAdding AutoShape... \nimage 1/1: 177x224 3 persons, 1 car, 1 stop sign\nSpeed: 8.8ms pre-process, 121.5ms inference, 1.2ms NMS per image at shape (1, 3, 512, 640)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-ipynb/cell-17-output-5.png){}\n:::\n:::\n\n\n## Summary \n\\\n\n- Neural networks are a flexible class of models.\n  - They are hard to train\n  - They are particular powerful for structured input like images, videos, audio, etc.\n- The good news is we can use pre-trained neural networks.\n  - This saves us a huge amount of time/cost/effort/resources.\n  - We can use these pre-trained networks directly or use them as feature transformers. \n\n---\njupyter:\n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n",
    "supporting": [
      "slides-03-deep-learning_files/figure-ipynb"
    ],
    "filters": []
  }
}