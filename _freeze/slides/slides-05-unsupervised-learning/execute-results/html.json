{
  "hash": "b50559bab1befbe1892cfb6c806f98b8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Unsupervised Learning\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\njupyter: \n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n\n\n## Learning outcomes \n\\\n\nFrom this module, you will be able to \n\n- Explain the unsupervised paradigm. \n- Explain the motivation and potential applications of clustering. \n- Define the clustering problem. \n- Broadly explain the K-Means algorithm and apply `sklearn`'s `KMeans` algorithm.  \n\n\n\n## Supervised learning\n\\\n\n- Training data comprises a set of observations ($X$) and their corresponding targets ($y$). \n- We wish to find a model function $f$ that relates $X$ to $y$.\n- Then use that model function to predict the targets of new examples.\n- We have been working with this set up so far. \n\n![](img/sup-learning.png)\n\n\n## Unsupervised learning\n\\\n\n- Training data consists of observations ($X$) without any corresponding targets.\n- Unsupervised learning could be used to group similar things together in $X$ or to find underlying structure in the data. \n\n![](img/unsup-learning.png)\n\n\n## Can we learn without targets?\n\\\n\n- Yes, but the learning will be focused on finding the underlying structures of the inputs themselves (rather than finding the function $f$ between input and output like we did in supervised learning models). \n\n- Examples:\n    - Clustering\n    - Dimensionality reduction\n\n\n## Labeled vs. Unlabeled data\n\n- If you have access to labeled training data, you're in the \"supervised\" setting. \n- You know what to do in that case from 571, 572, 573.  \n- Unfortunately, getting large amount of labeled training data is often time consuming and expensive.\n- Annotated data can become \"stale\" after a while in cases such as fraud detection. \n- Can you still make sense of the data even though you do not have the labels? \n- Yes! At least to a certain extent! \n\n## Clustering Activity (~10 mins)\n\\\n\nPick any of the two questions below and answer them in [this Google doc](https://docs.google.com/document/d/1TdmH5LKLC0Y9IWySC4FgsYX0dsNtkX_dBcc7FSSrQg0/edit?usp=sharing).\n\n![](img/food-clustering-activity.png)\n\n- Categorize the food items in the image and write your categories. Do you think there is one correct way to cluster these images? Why or why not?\n- If you want to build a machine learning model to cluster such images how would you represent such images?\n\n## What is clustering? \n\\\n\n**Clustering** is the task of partitioning the dataset into groups called clusters based on their similarities.\n\nThe goal of clustering is to discover underlying groups in a given dataset such that:\n- examples in the same group are as similar as possible;\n- examples in different groups are as different as possible.          \n\n\n## K-Means clustering \n\\\n\n- It requires us to specify the number of clusters in advance and each example is assigned to one (and only one) cluster.\n- The cluster centroids live in the same space as of the dataset but they are **not** actual data points, but instead are average points.\n- It always converges. Convergence is dependent upon the initial centers and it may converge to a sub-optimal solution. \n\n## K-Means toy dataset \n\\\n\n::: {#103e1a73 .cell execution_count=2}\n``` {.python .cell-code}\nX, y = make_blobs(n_samples=10, centers=3, n_features=2, random_state=10)\nmglearn.discrete_scatter(X[:, 0], X[:, 1]);\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-3-output-1.png){width=813 height=411}\n:::\n:::\n\n\n## K-Means demo\n\\\n\n![](img/k-means-iterative.png)\n\nK-Means is the simplest clustering algorithm. There is a variety of clustering algorithms available out there. \n\n## Let's cluster images!! \n\\\n\nFor this demo, I'm going to use two image datasets a small subset of [200 Bird Species with 11,788 Images](https://www.kaggle.com/datasets/veeralakrishna/200-bird-species-with-11788-images) dataset (available [here](../data/birds.zip))\n\n::: {#d3f3ba0d .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-4-output-1.png){width=763 height=318}\n:::\n:::\n\n\n# Clustering images with flattened representation \n## Flattening images\n\\\n\n::: {#669d6c56 .cell execution_count=4}\n``` {.python .cell-code}\nflatten_images = get_flattened_representations(data_dir, BATCH_SIZE)\nprint(\"Shape of the flattened images: \", flatten_images.shape) # 224 by 224 images with 3 color channels\nimage_shape=[3,224,224]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of the flattened images:  (176, 150528)\n```\n:::\n:::\n\n\n## K-Means on flattened representation\n\\\n\n::: {#6c9f3cc9 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nk = 3\nkm_flatten = KMeans(k, n_init='auto', random_state=123)\nkm_flatten.fit(flatten_images)\nprint(\"Shape of cluster centers: \", km_flatten.cluster_centers_.shape)\nunflatten_inputs = np.array([img.reshape(image_shape) for img in flatten_images])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of cluster centers:  (3, 150528)\n```\n:::\n:::\n\n\n## Examining clusters\n\\ \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#c648c942 .cell execution_count=6}\n``` {.python .cell-code}\nfor cluster in range(k):\n    get_cluster_images(km_flatten, flatten_images, unflatten_inputs, cluster, n_img=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n158\nImage indices:  [158  65  48 125  95]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-7-output-2.png){width=774 height=145}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n165\nImage indices:  [165  94  77 152 108]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-7-output-4.png){width=774 height=145}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n156\nImage indices:  [156  89 100  25 133]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-7-output-6.png){width=774 height=145}\n:::\n:::\n\n\n:::\n\n# Clustering images with representation extracted using transfer learning \n## Extract features using transfer learning\n\\\n\n::: {#a37e7eb1 .cell execution_count=7}\n``` {.python .cell-code}\ndensenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\ndensenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer\nZ_birds = get_features(densenet, birds_inputs)\nZ_birds.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(176, 1024)\n```\n:::\n:::\n\n\n## K-Means on extracted features\n\\\n\n::: {#771843bb .cell execution_count=8}\n``` {.python .cell-code}\nk = 3\nkm = KMeans(n_clusters=k, n_init='auto', random_state=123)\nkm.fit(Z_birds)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3, n_init=&#x27;auto&#x27;, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3, n_init=&#x27;auto&#x27;, random_state=123)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n## Examining clusters\n\\ \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#1420a4a5 .cell execution_count=9}\n``` {.python .cell-code}\nfor cluster in range(k):\n    get_cluster_images(km, Z_birds, X_birds, cluster, n_img=6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n103\nImage indices:  [103  23  86 162 168 122]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-10-output-2.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n55\nImage indices:  [55 31 53 15 88 84]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-10-output-4.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n120\nImage indices:  [120   5  11  14  22  69]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-10-output-6.png){width=782 height=130}\n:::\n:::\n\n\n:::\n\n## Let's try this on the food dataset\n\n\\\n\n::: {#ab77a132 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-11-output-1.png){width=763 height=318}\n:::\n:::\n\n\n## K-Means on food dataset\n\\\n\n::: {#14715cda .cell execution_count=11}\n``` {.python .cell-code}\nZ_food = get_features(densenet, food_inputs)\nk = 5\nkm = KMeans(n_clusters=k, n_init='auto', random_state=123)\nkm.fit(Z_food)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5, n_init=&#x27;auto&#x27;, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=5, n_init=&#x27;auto&#x27;, random_state=123)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n## Examining food clusters\n\\\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#c17df557 .cell execution_count=12}\n``` {.python .cell-code}\nfor cluster in range(k):\n    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n124\nImage indices:  [124  37 232  78 122 254]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-2.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n255\nImage indices:  [255 150 159   6  12 172]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-4.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n106\nImage indices:  [106 254 207 181 165 215]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-6.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n246\nImage indices:  [246 261 130 223 110 120]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-8.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n165\nImage indices:  [165  31  75  14  58  42]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-10.png){width=782 height=130}\n:::\n:::\n\n\n:::\n\n\n# Dimensionality reduction \n\n## Motivation \n\\\n\n- Representation plays a crucial role when you do machine learning. \n- How could we create meaningful representations? \n    - Dimensionality reduction is a popular approach! \n- Suppose you’re shown the picture below and you are told that this is Eva.\n- Do you have to remember every pixel in the image to recognize other pictures of Eva?\n\n![](img/eva-coffee.png)\n\n\n\n## Motivation \n\\\n\n- For example, if you are asked which one is Eva in the following pictures, it'll be fairly easy for you to identify her just based on some high-level features.\n- Just remembering important features such as shape of eyes, nose, mouth, shape and colour of hair etc. suffice to tell her apart from other people.\n\n![](img/hello-bmjs.png)\n\n## Dimensionality reduction activity\n\\\n\n- Goal Capture the essence of the classroom through photography.\n- We need two volunteers with cell phones to take photos of the classroom.\n- Volunteers will take pictures from various angles to ensure a comprehensive view of the classroom. Suggested angles include:\n    - An overhead view (from above)\n    - Views from each corner of the classroom\n    - A central perspective (from the center of the room)\n    - A low angle view (close to the floor)\n-  Among the photos taken, select and post the one that best captures the majority of objects and the overall atmosphere of the classroom in [this Google doc](https://docs.google.com/document/d/1TdmH5LKLC0Y9IWySC4FgsYX0dsNtkX_dBcc7FSSrQg0/edit#heading=h.trc35ybo6zyf)\n- Discuss: Why certain angles might be better than other for capturing more information?\n\n\n## Principal component analysis (PCA)\n\\\n\n- PCA is a popular technique for dimensionality reduction. \n- When going from higher dimensional space to lower dimensional space, it tries to capture the topology of the points in high dimensional space, making sure that we are not losing some of the important properties of the data.\n\n\n## PCA example\n\\\n\n- We'll work through a simpler dataset with grayscale images. \n- Let's look at some sample images from the dataset. \n\n::: {#1d4b4e51 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-14-output-1.png){width=950 height=390}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nThe shape of the dataset is:  (1500, 10000)\n```\n:::\n:::\n\n\n## Applying PCA\n\n::: {#6c5fa86f .cell execution_count=14}\n``` {.python .cell-code}\nn_components = 300 \npca = PCA(n_components=n_components, random_state=42)\npca.fit(X_anims);\n```\n:::\n\n\n## PCA components\n\n::: {#156660b4 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-16-output-1.png){width=795 height=501}\n:::\n:::\n\n\n## PCA components\n\\\n\n- We can express a data point (a cat image in this case) as a linear combination of principal components.  \n![](img/PCA-batcat-rep.png)\n\n## Reconstruction with varying number of components\n\\\n\n- When we extract components, we lose some information. To what extent can we reconstruct the original data point using a given number of components?\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#b9381981 .cell execution_count=16}\n``` {.python .cell-code}\nn_components = [10, 100, 200, 300, 500, 800]\nplot_pca_animals(X_anims, (100,100), n_components=n_components, index=30)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-17-output-1.png){width=1182 height=617}\n:::\n:::\n\n\n:::\n\n## Comments\n\\\n\nIt's possible to capture the essence of the dataset using far fewer features!\n\n\n## Word embeddings\n\n- This is not limited to images. \n- We can create useful representations for any type of data. \n- For example, an algorithm called `word2vec` creates representations for words so that similar words are close together in the vector space.  \n- You can also create [representation of sentences](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n\n![](img/t-SNE_word_embeddings.png)\n\n\n## Recommender systems (Optional)\n\n- The collaborative filtering approach to recommendation systems is based on a similar idea of identifying meaningful features of users and items.\n\n![](img/toy-movie-pattern.png)\n\n## Take-home message\n\n- A lot of data out there is unlabeled but we can still do many interesting things with it. \n\n",
    "supporting": [
      "slides-05-unsupervised-learning_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}