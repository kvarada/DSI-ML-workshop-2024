{
  "hash": "8eaabcfd996354d9ac0c7839b4d1b005",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Unsupervised Learning\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\njupyter: \n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n\n\n## Learning outcomes \n\\\n\nFrom this module, you will be able to \n\n- Explain the unsupervised paradigm. \n- Explain the motivation and potential applications of clustering. \n- Define the clustering problem. \n- Broadly explain the K-Means algorithm and apply `sklearn`'s `KMeans` algorithm.  \n\n\n\n## Supervised learning\n\\\n\n- Training data comprises a set of observations ($X$) and their corresponding targets ($y$). \n- We wish to find a model function $f$ that relates $X$ to $y$.\n- Then use that model function to predict the targets of new examples.\n- We have been working with this set up so far. \n\n![](img/sup-learning.png)\n\n\n## Unsupervised learning\n\\\n\n- Training data consists of observations ($X$) without any corresponding targets.\n- Unsupervised learning could be used to group similar things together in $X$ or to find underlying structure in the data. \n\n![](img/unsup-learning.png)\n\n\n## Can we learn without targets?\n\\\n\n- Yes, but the learning will be focused on finding the underlying structures of the inputs themselves (rather than finding the function $f$ between input and output like we did in supervised learning models). \n\n- Examples:\n    - Clustering\n    - Dimensionality reduction\n\n\n## Labeled vs. Unlabeled data\n\n- If you have access to labeled training data, you're in the \"supervised\" setting. \n- You know what to do in that case from 571, 572, 573.  \n- Unfortunately, getting large amount of labeled training data is often time consuming and expensive.\n- Annotated data can become \"stale\" after a while in cases such as fraud detection. \n- Can you still make sense of the data even though you do not have the labels? \n- Yes! At least to a certain extent! \n\n## Clustering Activity (~10 mins)\n\\\n\nPick any of the two questions below and answer them in [this Google doc](https://docs.google.com/document/d/1TdmH5LKLC0Y9IWySC4FgsYX0dsNtkX_dBcc7FSSrQg0/edit?usp=sharing).\n\n![](img/food-clustering-activity.png)\n\n- Categorize the food items in the image and write your categories. Do you think there is one correct way to cluster these images? Why or why not?\n- If you want to build a machine learning model to cluster such images how would you represent such images?\n\n## What is clustering? \n\\\n\n**Clustering** is the task of partitioning the dataset into groups called clusters based on their similarities.\n\nThe goal of clustering is to discover underlying groups in a given dataset such that:\n- examples in the same group are as similar as possible;\n- examples in different groups are as different as possible.          \n\n\n## K-Means clustering \n\\\n\n- It requires us to specify the number of clusters in advance and each example is assigned to one (and only one) cluster.\n- The cluster centroids live in the same space as of the dataset but they are **not** actual data points, but instead are average points.\n- It always converges. Convergence is dependent upon the initial centers and it may converge to a sub-optimal solution. \n\n## K-Means toy dataset \n\\\n\n::: {#346060fc .cell execution_count=2}\n``` {.python .cell-code}\nX, y = make_blobs(n_samples=10, centers=3, n_features=2, random_state=10)\nmglearn.discrete_scatter(X[:, 0], X[:, 1]);\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-3-output-1.png){width=813 height=411}\n:::\n:::\n\n\n## K-Means demo\n\\\n\n![](img/k-means-iterative.png)\n\n## DBSCAN (**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise)\n\\ \n\n- DBSCAN is a density-based clustering algorithm. \n- Intuitively, it's based on the idea that clusters form dense regions in the data and so it works by identifying \"crowded\" regions in the feature space. \n- It can address some of the limitations of K-Means we saw above. \n    - It does not require the user to specify the number of clusters in advance. \n    - It can identify points that are not part of any clusters. \n    - It can capture clusters of complex shapes. \n\n## DBSCAN idea\n\\ \n\n- Iterative algorithm.  \n- Based on the idea that clusters form dense regions in the data. \n\n![](img/DBSCAN_search.gif)\n\n[Source](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)\n\n\n## Hierarchical clustering \n\\\n\n\n\n## Let's cluster images!! \n\\\n\nFor this demo, I'm going to use two image datasets a small subset of [200 Bird Species with 11,788 Images](https://www.kaggle.com/datasets/veeralakrishna/200-bird-species-with-11788-images) dataset (available [here](../data/birds.zip))\n\n::: {#b0a5138a .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-4-output-1.png){width=763 height=318}\n:::\n:::\n\n\n# Clustering images with flattened representation \n## Flattening images\n\\\n\n::: {#b42c7848 .cell execution_count=4}\n``` {.python .cell-code}\nflatten_images = get_flattened_representations(data_dir, BATCH_SIZE)\nprint(\"Shape of the flattened images: \", flatten_images.shape) # 224 by 224 images with 3 color channels\nimage_shape=[3,224,224]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of the flattened images:  (176, 150528)\n```\n:::\n:::\n\n\n## K-Means on flattened representation\n\\\n\n::: {#e7a2cc30 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nk = 3\nkm_flatten = KMeans(k, n_init='auto', random_state=123)\nkm_flatten.fit(flatten_images)\nprint(\"Shape of cluster centers: \", km_flatten.cluster_centers_.shape)\nunflatten_inputs = np.array([img.reshape(image_shape) for img in flatten_images])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of cluster centers:  (3, 150528)\n```\n:::\n:::\n\n\n## Examining clusters\n\\ \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#caf3ce44 .cell execution_count=6}\n``` {.python .cell-code}\nfor cluster in range(k):\n    get_cluster_images(km_flatten, flatten_images, unflatten_inputs, cluster, n_img=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n158\nImage indices:  [158  65  48 125  95]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-7-output-2.png){width=774 height=145}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n165\nImage indices:  [165  94  77 152 108]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-7-output-4.png){width=774 height=145}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n156\nImage indices:  [156  89 100  25 133]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-7-output-6.png){width=774 height=145}\n:::\n:::\n\n\n:::\n\n# Clustering images with representation extracted using transfer learning \n## Extract features using transfer learning\n\\\n\n::: {#78b052a4 .cell execution_count=7}\n``` {.python .cell-code}\ndensenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\ndensenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer\nZ_birds = get_features(densenet, birds_inputs)\nZ_birds.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(176, 1024)\n```\n:::\n:::\n\n\n## K-Means on extracted features\n\\\n\n::: {#4d638574 .cell execution_count=8}\n``` {.python .cell-code}\nk = 3\nkm = KMeans(n_clusters=k, n_init='auto', random_state=123)\nkm.fit(Z_birds)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3, n_init=&#x27;auto&#x27;, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3, n_init=&#x27;auto&#x27;, random_state=123)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n## Examining clusters\n\\ \n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#31c38386 .cell execution_count=9}\n``` {.python .cell-code}\nfor cluster in range(k):\n    get_cluster_images(km, Z_birds, X_birds, cluster, n_img=6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n103\nImage indices:  [103  23  86 162 168 122]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-10-output-2.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n55\nImage indices:  [55 31 53 15 88 84]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-10-output-4.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n120\nImage indices:  [120   5  11  14  22  69]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-10-output-6.png){width=782 height=130}\n:::\n:::\n\n\n:::\n\n## Let's try this on the food dataset\n\n\\\n\n::: {#a7656a98 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-11-output-1.png){width=763 height=318}\n:::\n:::\n\n\n## K-Means on food dataset\n\\\n\n::: {#bb3991cc .cell execution_count=11}\n``` {.python .cell-code}\nZ_food = get_features(densenet, food_inputs)\nk = 5\nkm = KMeans(n_clusters=k, n_init='auto', random_state=123)\nkm.fit(Z_food)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5, n_init=&#x27;auto&#x27;, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=5, n_init=&#x27;auto&#x27;, random_state=123)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n## Examining food clusters\n\\\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n\n::: {#8e6b1eb3 .cell execution_count=12}\n``` {.python .cell-code}\nfor cluster in range(k):\n    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n124\nImage indices:  [124  37 232  78 122 254]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-2.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n255\nImage indices:  [255 150 159   6  12 172]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-4.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n106\nImage indices:  [106 254 207 181 165 215]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-6.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n246\nImage indices:  [246 261 130 223 110 120]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-8.png){width=782 height=130}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n165\nImage indices:  [165  31  75  14  58  42]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-05-unsupervised-learning_files/figure-revealjs/cell-13-output-10.png){width=782 height=130}\n:::\n:::\n\n\n:::\n\n\n# Dimensionality reduction \n\n## Motivation \n\\\n\n- Representation plays a crucial role when you do machine learning. \n- How could we create meaningful representations? \n\n\n## Word embeddings\n\n## Recommender systems \n\n",
    "supporting": [
      "slides-05-unsupervised-learning_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}