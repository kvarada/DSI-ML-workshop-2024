{
  "hash": "03bfecdcdf23463db9d5402d93694246",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Supervised Machine Learning Models\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\njupyter: \n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n\n\n## Supervised Learning\n\\\n\nIn the next section, we will briefly introduce a few types of machine learning models that are often used for supervised learning tasks.\n\nWe will discuss some basic intuition around how they work, and also discuss their relative strengths and shortcomings.\n\n# Tree-based models\n\n## Tree-based models\n\n![](img/actual-tree.jpg)\n\nWe have seen that decision trees are prone to overfitting. There are several models that extend the basic idea of using decision trees.\n\n## Random Forest\n\n![](img/random-forest.png)\n\nTrain an ensemble of distinct decision trees.\n\n## Random Forest\n\\\n\nEach tree trains on a random sample of the data. Some times the features used to split are also randomized at each node.\n\nIdea: Individual trees still learn noise in the data, but the noise should \"average out\" over the ensemble.\n\n## Gradient Boosted Trees\n\n![](img/boost.png)\n\nEach tree tries to \"correct\" or improve the previous tree's prediction.\n\n## Tree-Based Models\n\\ \n\nRandom Forest, XGBoost, etc are all easily available as \"out-of-the box solutions\".\n\nPros: \n\n* Perform well on a variety of tasks\n* Random forest in particular are easy to train and robust to outliers.\n\nCons:\n\n* Not always interpretable\n* Not good at handling sparse data\n* Can also still overfit.\n\n# Analogy-based models\n\n## Analogy-based models\n\n![](img/us-map.png)\n\nCan you guess what this dataset is?\n\n## Analogy-based models\n\n![](img/us-map-dot.jpg)\n\nHow would you classify the green dot?\n\n## Analogy-based models\n\\\n\nIdea: predict on new data based on \"similar\" examples in the training data.\n\n## *K*-Nearest-Neighbour Classifier\n\\\n\nFind the *K* nearest neighbours of an example, and predict whichever class was most common among them.\n\n'*K*' is a hyperparameter. Choosing *K=1* is likely to overfit. If the dataset has *N* examples, setting *K=N* just predicts the mode (dummy classifier).\n\nNo training phase, but the model can get arbitrarily large (and take very long to make predictions).\n\n## SVM with RBF kernel\n\\ \n\nAnother 'analogy-based' classification method.\n\nThe model stores examples with positive and negative weights. Being close to a positive example makes your label more likely to be positive.\n\nCan lead to \"smoother\" decision boundaries than K-NNs, and potentially to a smaller trained model.\n\n## KNNs and SVMs\n\\ \n![](img/knn-vs-svm.png)\n\n## Analogy-based Models\n\\\n\nPros:\n\n* Do not need to make assumptions about the underlying data\n* Given enough data, should pretty much always work.\n\nCons:\n\n* *Enough* data can mean ... a lot\n* Computing distances is time-consuming for large datasets\n* Can't really interpret the model's decisions.\n\n# Linear models\n\n## Linear models\n\\\n\n![](img/lse)\n\nMany of you might be familiar with least-squares regression. We find the line of best fit by minimizing the 'squared error' of the predictions.\n\n## Linear Models\n\\\n\n![](img/quadreg.png)\n\nWe can use the same idea (and some neat tricks) to fit higher degree polynomials, or any chosen basis of functions.\n\n## Linear Models\n\\ \n![](img/outlierbad.png)\n\nSquared Error is very sensitive to outliers. Far-away points contribute a very large squared error, and even relatively few points can affect the outcome.\n\n## Linear Models\n\\ \n![](img/outliergood.png)\n\nWe can use other notions of \"best fit\". Using absolute error makes the model more resistant to outliers!\n\n## Linear Classifiers\n\\\n\nWe can also build linear models for classification tasks. The idea is to convert the output from an arbitrary number to a number between 0 and 1, and treat it like a \"probability\".\n\nIn *logistic regression*, we squash the output using the sigmoid function and then adjust parameters (in training) to find the choice that makes the data \"most likely\".\n\n## Linear Classifiers\n\n![](img/logistic.png)\n\nLogistic Regression predicts a *linear* decision boundary.\n\n## Linear Classifiers\n\\\n\nPros:\n\n* Easy to train and to interpret\n* Widely applicable despite some strong assumptions\n* If you have a regression task, check whether a linear regression is already good enough! If you have a classification task, logistic regression is a go-to first option.\n\nCons:\n\n* Strong assumptions\n* Linear decision boundaries for classifiers\n* Correlated features can cause problems\n\n## A Look Ahead\n\\\n\nSupport Vector Machines (SVM) are also linear classifiers.\n\nThe reason we saw a *non-linear* decision boundary earlier was the use of the *RBF* kernel, which applies a certain non-linear transformation to the features.\n\nEven if our data is not linearly separable, there could be a good choice of feature transform out there that *makes* it linearly separable.\n\n##\n\nWouldn't it be nice if we could train a machine learning model to find such a transform?\n\n",
    "supporting": [
      "slides-03-ml-models_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}