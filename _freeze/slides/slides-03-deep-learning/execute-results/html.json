{
  "hash": "50aff4ab7af9e2cce32ed41acae57bfa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deep Learning\"\nformat: \n    revealjs:\n      smaller: true\n      center: true\njupyter: \n  kernelspec:\n    display_name: '571'\n    language: python\n    name: '571'\n---\n\n\n\n## Learning outcomes \n\\\nFrom this module, you will be able to \n\n- Explain the role of neural networks in machine learning, including their advantages and disadvantages.\n- Discuss why traditional methods are less effective for image data.\n- Gain a high-level understanding of transfer learning.\n- Differentiate between image classification and object detection.\n\n## Introduction to neural networks\n\\\n\n- Neural networks are very popular these days under the name deep learning.\n- Neural networks apply a sequence of transformations on your input data.\n- They can be viewed a generalization of linear models where we apply a series of transformations.\n- Here is graphical representation of a logistic regression model.\n- We have 4 features: x[0], x[1], x[2], x[3]\n\n::: {#7d611ee1 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](slides-03-deep-learning_files/figure-revealjs/cell-3-output-1.svg){}\n:::\n:::\n\n\n## Adding a layer of transformations \n\\\n\n- Below we are adding one \"layer\" of transformations in between features and the target. \n- We are repeating the the process of computing the weighted sum multiple times.  \n- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. \n\n::: {#5df282bd .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](slides-03-deep-learning_files/figure-revealjs/cell-4-output-1.svg){}\n:::\n:::\n\n\n## One more layer of transformations \n\\\n\n- Now we are adding one more layer of transformations. \n\n::: {#8246fa8d .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](slides-03-deep-learning_files/figure-revealjs/cell-5-output-1.svg){}\n:::\n:::\n\n\n## Neural networks \n\\\n\n- A neural network is a model that's sort of like its own pipeline\n  - It involves a series of transformations (\"layers\") internally. \n  - The output is the prediction.\n\n- With a neural net, you specify the number of features after each transformation.\n  - In the above, it goes from 4 to 3 to 3 to 1.\n\n- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. \n- Neural network = neural net\n- Deep learning ~ using neural networks\n\n## Why neural networks?\n\\\n\n- They can learn very complex functions.\n  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.\n  - More layers / bigger layers --> more complex model.\n  - You can generally get a model that will not underfit. \n\n- The work really well for structured data:\n  - 1D sequence, e.g. timeseries, language\n  - 2D image\n  - 3D image or video\n- They've had some incredible successes in the last 10 years.\n- Transfer learning (coming later today) is really useful.  \n\n## Why not neural networks?\n\\\n\n- Often they require a lot of data.\n- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n- They have huge numbers of hyperparameters are a huge pain to tune.\n  - Think of each layer having hyperparameters, plus some overall hyperparameters.\n  - Being slow compounds this problem.\n- They are not interpretable.\n- I don't recommend training them on your own without further training\n\n  - Good news\n    - You don't have to train your models from scratch in order to use them.\n    - I'll show you some ways to use neural networks without training them yourselves. \n\n## Deep learning software\n\\\n\nThe current big players are:\n\n1. [PyTorch](http://pytorch.org)\n2. [TensorFlow](https://www.tensorflow.org)\n\nBoth are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n\n<br><br>\n\n## Introduction to computer vision\n\\\n\n- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. It has many tasks of interest:\n    - image classification: is this a cat or a dog?\n    - object localization: where is the cat in this image?\n    - object detection: What are the various objects in the image? \n    - instance segmentation: What are the shapes of these various objects in the image? \n    - and much more...\n\n![](img/vision-apps.jpeg)\n\n<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> \n\nIn the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.\n\n## Image classification\n\\\n\nHave you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\nThis can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.\n\n## Image classification\n\\\n\nImage classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.\n\n![](img/cat_variation.png)\n<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->\n\n## Convolutional Neural Networks\n\\\n\n- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. \n- [ImageNet Classification with Deep Convolutional\nNeural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n- Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. \n\n## Pre-trained models\n\\\n\n- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\n- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.\n- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\n- It refers to using a model already trained on one task as a starting point for learning to perform another task\n\n## Pre-trained models\n\\\n\n- There are many famous deep learning architectures out there that have been very successful across a wide range of problems, e.g.: [AlexNet](https://arxiv.org/abs/1404.5997), [VGG](https://arxiv.org/abs/1409.1556), [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/abs/1512.00567), [MobileNet](https://arxiv.org/abs/1801.04381), etc.\n\n- Many of these models have been pre-trained on famous datasets like ImageNet [[1](https://www.image-net.org/index.php), [2](https://en.wikipedia.org/wiki/ImageNet)]\n\n## ImageNet\n\\\n\n- [ImageNet](http://www.image-net.org/) is an image dataset that became a very popular benchmark in the field ~12 years ago. \n- Currently contains ~14 million labelled images with ~21,841 categories  \n- There are various versions with different number of images and classes\n    - ILSVRC, a popular annual competition in computer vision, uses a smaller subset of ImageNet. This subset consists of about 1.2 million training images, 50,000 validation images, and 150,000 testing images across 1,000 categories. \n- [Wikipedia article](https://en.wikipedia.org/wiki/ImageNet) on ImageNet\n\n## ImageNet classes \n\\\n\n- Here are some example classes. \n\n::: {#b3e11e32 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n['black swan, Cygnus atratus',\n 'tusker',\n 'echidna, spiny anteater, anteater',\n 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n 'wallaby, brush kangaroo',\n 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n 'wombat',\n 'jellyfish',\n 'sea anemone, anemone',\n 'brain coral']\n```\n:::\n:::\n\n\n## Transfer learning \n\\\n\n- The idea of transfer learning is instead of developing a machine learning model from scratch, you use these available pre-trained models for your tasks either directly or by fine tuning them. \n- There are three common ways to use transfer learning in computer vision \n    1. Using pre-trained models out-of-the-box \n    2. Using pre-trained models as feature extractor and training your own model with these features\n    2. Starting with weights of pre-trained models and fine-tuning the weights for your task. \n- We will explore the first two approaches.     \n\n## Using pre-trained models out-of-the-box \n\\\n\n![](img/cnn-ex.png)\n\n<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->\n\n- Let's first try one of these models and apply it to our own problem right out of the box. \n\n\n## Using pre-trained models out-of-the-box \n\\\n\n- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)\n- We used a pre-trained model vgg16 which is trained on the ImageNet data. \n- We preprocess the given image. \n- We get prediction from this pre-trained model on a given image along with prediction probabilities.  \n- For a given image, this model will spit out one of the 1000 classes from ImageNet. \n\n## Using pre-trained models out-of-the-box {.scrollable}\n\n- Let's predict labels with associated probabilities for unseen images\n\n::: {#b8e127d5 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-7-output-1.png){width=434 height=325}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\n                     tiger cat              0.353\n              tabby, tabby cat              0.207\n               lynx, catamount              0.050\nPembroke, Pembroke Welsh corgi              0.046\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-7-output-3.png){width=324 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.983\n                  leopard, Panthera pardus              0.012\njaguar, panther, Panthera onca, Felis onca              0.004\n       snow leopard, ounce, Panthera uncia              0.001\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-7-output-5.png){width=309 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                   Class  Probability score\n                                 macaque              0.714\npatas, hussar monkey, Erythrocebus patas              0.122\n      proboscis monkey, Nasalis larvatus              0.098\n                   guenon, guenon monkey              0.017\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-7-output-7.png){width=299 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                        Class  Probability score\nWalker hound, Walker foxhound              0.580\n             English foxhound              0.091\n                  EntleBucher              0.080\n                       beagle              0.065\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n## Using pre-trained models out-of-the-box \n\\\n\n- We got these predictions without \"doing the ML ourselves\".\n- We are using **pre-trained** `vgg16` model which is available in `torchvision`.\n  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n- Many of these models have been pre-trained on famous datasets like **ImageNet**. \n- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. \n\n## Using pre-trained models out-of-the-box {.scrollable}\n\\\n\n- Let's try some images which are unlikely to be there in ImageNet. \n- It's not doing very well here because ImageNet doesn't have proper classes for these images.\n\n::: {#5fa6da2a .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-1.png){width=549 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n         Class  Probability score\ncucumber, cuke              0.146\n         plate              0.117\n     guacamole              0.099\n  Granny Smith              0.091\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-3.png){width=330 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Class  Probability score\nchocolate sauce, chocolate syrup              0.609\n                        espresso              0.039\n             ice cream, icecream              0.037\n                     face powder              0.030\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-5.png){width=702 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      Class  Probability score\n                                        fig              0.637\n                                pomegranate              0.193\ngrocery store, grocery, food market, market              0.041\n                                      crate              0.023\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-7.png){width=611 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                               Class  Probability score\n                                         toilet seat              0.171\n                                          safety pin              0.060\nbannister, banister, balustrade, balusters, handrail              0.039\n                                              bubble              0.035\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-9.png){width=572 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                    Class  Probability score\nrobin, American robin, Turdus migratorius              0.203\n                                  jacamar              0.156\n                                   bulbul              0.122\n      brambling, Fringilla montifringilla              0.103\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-11.png){width=609 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                         Class  Probability score\ngoldfinch, Carduelis carduelis              0.585\n                     bee eater              0.219\n                        toucan              0.038\n                      hornbill              0.026\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-13.png){width=428 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Class  Probability score\n                   vase              0.078\n                thimble              0.074\n             plate rack              0.049\nsaltshaker, salt shaker              0.047\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-15.png){width=331 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                      Class  Probability score\n           pizza, pizza pie              0.998\nfrying pan, frypan, skillet              0.001\n                     potpie              0.000\n                French loaf              0.000\n--------------------------------------------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-8-output-17.png){width=607 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n              Class  Probability score\n     patio, terrace              0.213\n           fountain              0.164\nlakeside, lakeshore              0.097\n            sundial              0.088\n--------------------------------------------------------------\n```\n:::\n:::\n\n\n## Using pre-trained models out-of-the-box\n\\\n\n- Here we are using pre-trained models out-of-the-box. \n- Can we use pre-trained models for our own classification problem with our classes? \n- Yes!! We have two options here:\n    1. Add some extra layers to the pre-trained network to suit our particular task\n    2. Pass training data through the network and save the output to use as features for training some other model\n\n\n## Using pre-trained models to extract features \n\\\n\n- Let's use pre-trained models to extract features.\n- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. \n- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. \n- You can think of each layer a transformer applying some transformations on the input received to that later. \n\n![](img/cnn-ex.png)\n\n\n## Using pre-trained models to extract features \n\\\n\n- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. \n- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  \n- Let's try this out. \n- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. \n\n## Using pre-trained models to extract features \n\\\n\nLet's look at some sample images in the dataset. \n\n::: {#c418aeea .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-9-output-1.png){width=611 height=631}\n:::\n:::\n\n\n## Dataset statistics\n\\\n\nHere is the stat of our toy dataset. \n\n::: {#3122f1e3 .cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses: ['beet_salad', 'chocolate_cake', 'edamame', 'french_fries', 'pizza', 'spring_rolls', 'sushi']\nClass count: 40, 38, 40\nSamples: 283\nFirst sample: ('data/food/train/beet_salad/104294.jpg', 0)\n```\n:::\n:::\n\n\n## Using pre-trained models to extract features \n\\\n\n- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  \n\n\n\n## Shape of the feature vector\n\\\n\n- Now we have extracted feature vectors for all examples. What's the shape of these features?\n\n::: {#9697a1b7 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\ntorch.Size([283, 1024])\n```\n:::\n:::\n\n\n- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  \n\n![](img/densenet-architecture.png)\n\n[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)\n\n## A feature vector given by densenet \n\\ \n\n- Let's examine the feature vectors. \n\n::: {#e5f3f9e2 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1014</th>\n      <th>1015</th>\n      <th>1016</th>\n      <th>1017</th>\n      <th>1018</th>\n      <th>1019</th>\n      <th>1020</th>\n      <th>1021</th>\n      <th>1022</th>\n      <th>1023</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000397</td>\n      <td>0.006987</td>\n      <td>0.002080</td>\n      <td>0.001633</td>\n      <td>0.164544</td>\n      <td>1.284627</td>\n      <td>0.000326</td>\n      <td>0.003747</td>\n      <td>0.019033</td>\n      <td>0.000411</td>\n      <td>...</td>\n      <td>2.017399</td>\n      <td>0.265347</td>\n      <td>4.620860</td>\n      <td>0.285282</td>\n      <td>0.526914</td>\n      <td>0.428760</td>\n      <td>2.960556</td>\n      <td>1.577721</td>\n      <td>2.150136</td>\n      <td>0.735927</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000639</td>\n      <td>0.004926</td>\n      <td>0.000985</td>\n      <td>0.001324</td>\n      <td>0.145076</td>\n      <td>0.558028</td>\n      <td>0.000775</td>\n      <td>0.002603</td>\n      <td>0.181858</td>\n      <td>0.000249</td>\n      <td>...</td>\n      <td>0.158791</td>\n      <td>0.380575</td>\n      <td>1.309546</td>\n      <td>0.009790</td>\n      <td>0.007745</td>\n      <td>1.773695</td>\n      <td>0.298152</td>\n      <td>0.697784</td>\n      <td>0.462329</td>\n      <td>1.448314</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000109</td>\n      <td>0.006808</td>\n      <td>0.003910</td>\n      <td>0.002917</td>\n      <td>0.137208</td>\n      <td>0.702670</td>\n      <td>0.000546</td>\n      <td>0.003634</td>\n      <td>0.386037</td>\n      <td>0.000179</td>\n      <td>...</td>\n      <td>1.730926</td>\n      <td>0.139264</td>\n      <td>0.384295</td>\n      <td>1.754405</td>\n      <td>0.267863</td>\n      <td>0.480650</td>\n      <td>0.591290</td>\n      <td>3.880835</td>\n      <td>1.436777</td>\n      <td>0.856637</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000350</td>\n      <td>0.002965</td>\n      <td>0.001391</td>\n      <td>0.000091</td>\n      <td>0.119358</td>\n      <td>0.032185</td>\n      <td>0.000413</td>\n      <td>0.004407</td>\n      <td>0.000000</td>\n      <td>0.000120</td>\n      <td>...</td>\n      <td>0.866084</td>\n      <td>0.118614</td>\n      <td>0.404704</td>\n      <td>0.139143</td>\n      <td>0.588674</td>\n      <td>0.326834</td>\n      <td>0.277743</td>\n      <td>2.369123</td>\n      <td>0.295956</td>\n      <td>0.206887</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000350</td>\n      <td>0.003532</td>\n      <td>0.002254</td>\n      <td>0.002841</td>\n      <td>0.102675</td>\n      <td>0.382482</td>\n      <td>0.000436</td>\n      <td>0.004212</td>\n      <td>0.816037</td>\n      <td>0.000554</td>\n      <td>...</td>\n      <td>0.010685</td>\n      <td>1.147244</td>\n      <td>1.892542</td>\n      <td>1.929401</td>\n      <td>0.694761</td>\n      <td>0.744421</td>\n      <td>1.705219</td>\n      <td>1.241349</td>\n      <td>2.351263</td>\n      <td>0.054507</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1024 columns</p>\n</div>\n```\n:::\n:::\n\n\n- The features are hard to interpret but they have some important information about the images which can be useful for classification.  \n\n## Logistic regression with the extracted features \n\\\n\n- Let's try out logistic regression on these extracted features. \n\n::: {#687fea71 .cell execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score:  1.0\n```\n:::\n:::\n\n\n::: {#1dd4eb13 .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation score:  0.8507462686567164\n```\n:::\n:::\n\n\n- This is great accuracy for so little data and little effort!!!\n\n\n## Sample predictions\n\\\n\nLet's examine some sample predictions on the validation set.  \n\n::: {#f4c977d0 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-16-output-1.png){width=1151 height=1901}\n:::\n:::\n\n\n## Object detection using [YOLO](https://docs.ultralytics.com/)\n\\\n\n- Another useful task and tool to know is object detection using YOLO model. \n- Let's identify objects in a sample image using a pretrained model called YOLO5\n\n::: {#ef1a3f86 .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\nCollecting ultralytics\n  Obtaining dependency information for ultralytics from https://files.pythonhosted.org/packages/fb/3f/c58aea90d742cd736b254d9a82aed389641cca63893c19b831dc6978e0ef/ultralytics-8.2.79-py3-none-any.whl.metadata\n  Downloading ultralytics-8.2.79-py3-none-any.whl.metadata (41 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/41.3 kB ? eta -:--:--\r     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 1.8 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (1.26.0)\nRequirement already satisfied: matplotlib>=3.3.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (3.8.0)\nRequirement already satisfied: opencv-python>=4.6.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (4.10.0)\nRequirement already satisfied: pillow>=7.1.2 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (10.2.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (1.11.3)\nRequirement already satisfied: torch>=1.8.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (2.3.1)\nRequirement already satisfied: torchvision>=0.9.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (0.18.1a0)\nRequirement already satisfied: tqdm>=4.64.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (5.9.5)\nCollecting py-cpuinfo (from ultralytics)\n  Obtaining dependency information for py-cpuinfo from https://files.pythonhosted.org/packages/e0/a9/023730ba63db1e494a271cb018dcd361bd2c917ba7004c3e49d5daf795a2/py_cpuinfo-9.0.0-py3-none-any.whl.metadata\n  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\nRequirement already satisfied: pandas>=1.1.4 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from ultralytics) (2.1.1)\nCollecting seaborn>=0.11.0 (from ultralytics)\n  Obtaining dependency information for seaborn>=0.11.0 from https://files.pythonhosted.org/packages/83/11/00d3c3dfc25ad54e731d91449895a79e4bf2384dc3ac01809010ba88f6d5/seaborn-0.13.2-py3-none-any.whl.metadata\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Obtaining dependency information for ultralytics-thop>=2.0.0 from https://files.pythonhosted.org/packages/8f/34/08f80cc72ece5ecac4a910b6a572d8bef46d4d9dc37be2a4c8425e6cae75/ultralytics_thop-2.0.5-py3-none-any.whl.metadata\n  Downloading ultralytics_thop-2.0.5-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\nRequirement already satisfied: cycler>=0.10 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.43.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.0.5)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\nRequirement already satisfied: filelock in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.12.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\nRequirement already satisfied: sympy in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1)\nRequirement already satisfied: jinja2 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\nRequirement already satisfied: six>=1.5 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.2.79-py3-none-any.whl (869 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/869.1 kB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 860.2/869.1 kB 26.5 MB/s eta 0:00:01\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 869.1/869.1 kB 20.4 MB/s eta 0:00:00\nDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/294.9 kB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 24.3 MB/s eta 0:00:00\nDownloading ultralytics_thop-2.0.5-py3-none-any.whl (25 kB)\nDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\nInstalling collected packages: py-cpuinfo, ultralytics-thop, seaborn, ultralytics\nSuccessfully installed py-cpuinfo-9.0.0 seaborn-0.13.2 ultralytics-8.2.79 ultralytics-thop-2.0.5\nrequirements: Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\nCollecting gitpython>=3.1.30\n  Obtaining dependency information for gitpython>=3.1.30 from https://files.pythonhosted.org/packages/e9/bd/cc3a402a6439c15c3d4294333e13042b915bbeab54edc457c723931fed3f/GitPython-3.1.43-py3-none-any.whl.metadata\n  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\nCollecting pillow>=10.3.0\n  Obtaining dependency information for pillow>=10.3.0 from https://files.pythonhosted.org/packages/0e/69/a31cccd538ca0b5272be2a38347f8839b97a14be104ea08b0db92f749c74/pillow-10.4.0-cp310-cp310-macosx_10_10_x86_64.whl.metadata\n  Downloading pillow-10.4.0-cp310-cp310-macosx_10_10_x86_64.whl.metadata (9.2 kB)\nCollecting requests>=2.32.0\n  Obtaining dependency information for requests>=2.32.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting setuptools>=70.0.0\n  Obtaining dependency information for setuptools>=70.0.0 from https://files.pythonhosted.org/packages/07/6a/0270e295bf30c37567736b7fca10167640898214ff911273af37ddb95770/setuptools-73.0.1-py3-none-any.whl.metadata\n  Downloading setuptools-73.0.1-py3-none-any.whl.metadata (6.6 kB)\nCollecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.32.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.32.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.32.0) (2.0.5)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages (from requests>=2.32.0) (2024.7.4)\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\nDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/207.3 kB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 9.4 MB/s eta 0:00:00\nDownloading pillow-10.4.0-cp310-cp310-macosx_10_10_x86_64.whl (3.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.5 MB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 1.5/3.5 MB 44.6 MB/s eta 0:00:01\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 58.2 MB/s eta 0:00:00\nDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/64.9 kB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 11.5 MB/s eta 0:00:00\nDownloading setuptools-73.0.1-py3-none-any.whl (2.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.3 MB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 69.3 MB/s eta 0:00:00\nDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/62.7 kB ? eta -:--:--\r   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 152.0 MB/s eta 0:00:00\nDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\nInstalling collected packages: smmap, setuptools, requests, pillow, gitdb, gitpython\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 68.2.2\n    Uninstalling setuptools-68.2.2:\n      Successfully uninstalled setuptools-68.2.2\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pillow\n    Found existing installation: pillow 10.2.0\n    Uninstalling pillow-10.2.0:\n      Successfully uninstalled pillow-10.2.0\nSuccessfully installed gitdb-4.0.11 gitpython-3.1.43 pillow-10.4.0 requests-2.32.3 setuptools-73.0.1 smmap-5.0.1\n\nrequirements: AutoUpdate success ✅ 4.1s, installed 4 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0', 'requests>=2.32.0', 'setuptools>=70.0.0']\nrequirements: ⚠️ Restart runtime or rerun command for updates to take effect\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-03-deep-learning_files/figure-revealjs/cell-17-output-2.png){}\n:::\n:::\n\n\n## Summary \n\\\n\n- Neural networks are a flexible class of models.\n  - They are hard to train\n  - They are particular powerful for structured input like images, videos, audio, etc.\n- The good news is we can use pre-trained neural networks.\n  - This saves us a huge amount of time/cost/effort/resources.\n  - We can use these pre-trained networks directly or use them as feature transformers. \n\n",
    "supporting": [
      "slides-03-deep-learning_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}